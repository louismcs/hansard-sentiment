% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{placeins}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{float}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{mdframed}

\definecolor{light-gray}{gray}{0.95}
\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\newcommand{\mylisting}[4]{\lstinputlisting[language=#1, basicstyle=\scriptsize, caption={#3}, label={lst:#4}, upquote=true, captionpos=b, float, floatplacement=H, backgroundcolor=\color{light-gray}, showstringspaces=false]{listings/#4.#2}}

\newcommand{\pylisting}[2]{\mylisting{Python}{py}{#1}{#2}}

\newcommand{\htmllisting}[2]{\mylisting{HTML}{html}{#1}{#2}}

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Louis Max Cley Slater}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{Sentiment Analysis of Texts on the Iraq War} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Pembroke College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Louis Max Cley Slater                       \\
College:            & \bf Pembroke College                     \\
Project Title:      & \bf Sentiment Analysis of Texts on the Iraq War \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2018  \\
Word Count:         & \textbf{11,887}  (calculated using TeXstudio)\\
Project Originator: & Louis Max Cley Slater                    \\
Supervisor:         & Dr Tamara Polajnar                    \\ 
\end{tabular}
}


\section*{Original Aims of the Project}

Due to my interest in both natural language processing and politics and a lack of previous work done in the area, I decided that I wanted to perform sentiment analysis on British political texts - in particular, texts on the Iraq war.

\section*{Work Completed}
In this project, I developed two components of a system to perform sentiment analysis on texts on the Iraq war. The first component constructs a dataset of speeches and votes in the House of Commons, while the second is a support vector machine classifier used to determine the stance of a given speech on the 2003 invasion of Iraq. The classifier is shown to achieve an F1 score of 0.894 on the test data, which is a far better performance than the project's baseline classifier.


\section*{Special Difficulties}
There were licensing issues with the dataset I initially planned to use, meaning that I had to construct my own dataset before implementing the project's classifier. 
\newpage

\section*{Declaration}

I, Louis Max Cley Slater of Pembroke College, being a candidate for Part II of the Computer Science Tripos, hereby declare that this dissertation and the work described in it are my own work, unaided except as may be specified below, and that the dissertation does not contain material that has already been used to any substantial extent for a comparable purpose.

\bigskip
\leftline{Signed:}
\vspace{5pt}
\includegraphics[scale=0.3]{figs/signature.png}
\vspace{5pt}
\\
\medskip
\leftline{Date: 17/05/2018} 

\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}
\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}

Thank you so much to everyone who has supported me in any way with this project. Particular thanks to Tamara Polajnar for being an amazing supervisor, to Anil Madhavapeddy for being an amazing Director of Studies and to my friends and family for just being amazing.

\pagestyle{headings}

\chapter{Introduction}

This dissertation describes the implementation of the two components of a system to perform sentiment analysis on texts on the Iraq war. The first component constructs a dataset of speeches and votes in the House of Commons. The second is a support vector machine classifier used to determine the stance of a given speech on the Iraq war. The classifier achieves an F1 score of 0.894 on the test data, which is a far better performance than the na\"{i}ve Bayes classifier, which is used as the project's baseline.

\section{Problem Formulation} \label{intro-challenges}

In this project, I look at how best to apply machine learning techniques to sentiment analysis on texts about the Iraq war. To optimise the performance of the sentiment analysis, I created a dataset using the Hansard (which is a collection of transcripts of House of Commons debates) and data from data.parliament.uk. I developed this dataset in a way that made it easy to label an MP's speeches using their voting record, thus giving a labelled corpus (a corpus is a collection of written or spoken material stored on a computer and used to find out how language is used \cite{corpus_definition}). This meant that I could consider the sentiment analysis as a supervised learning classification problem. One of the simplest implementations of a classifier is the na\"{i}ve Bayes classifier (described in \S\ref{prep-bayes}), so I use this as a baseline. For reasons outlined in \S\ref{prep-svm}, I use a support vector machine classifier as the default model. Throughout this dissertation, I discuss how I optimised the classifier while maintaining good software engineering practices.

\section{Motivation}

The Iraq war was an issue that cut across the political spectrum, making it an interesting topic for a sentiment analysis project, since someone's stance on the war cannot be easily determined from their views on other issues \cite{mp_votes_bbc}. The recent publication of the Iraq Inquiry (commonly known as the Chilcot Inquiry) \cite{chilcot2016report} and the ongoing situation in Iraq and Syria \cite{syria_iraq_air_strikes} mean that such a project is particularly timely.
\\\\
The development and publication of the project's dataset will facilitate further NLP work on House of Commons data, which will hopefully result in increased accountability of MPs.


\section{Related Work} \label{intro-related}
There have been various NLP studies which have used the Hansard. The 2014 work by Wattam et al. is most similar to my use of the Hansard, since it also develops a new dataset. The dataset they construct is a tagged version of the Hansard, which facilitates faster retrieval of the data \cite{hansardtagging}.
\\\\
While many sentiment analysis studies have been based on political issues, since 2009 the majority of such research has concerned Tweets. The first study to use Twitter as its primary corpus was `Twitter power: Tweets as electronic word of mouth' \cite{first_twitter} and there have since been countless similar studies. In 2010, Pak et al. proposed that Twitter could be used to determine public opinion \cite{twitter_as_a_corpus}, which was proven true later that year when sentiment analysis of Twitter provided predictions that paralleled the results of traditional election polls for the German federal election \cite{predicting_elections_twitter}. This focus on Twitter is useful, but since political decisions are made in Government and not on the internet, we should also use computational methods to learn more about how our MPs represent us in Parliament. Although it makes the project more interesting, analysing longer political texts presents more challenges than analysing Tweets, in part due to the lack of guidance from previous work.
\\\\
In 2016, Zhou et al. published a paper on their work in which they performed sentiment analysis of Wikipedia articles on wars by comparing Wikipedia pages in different languages \cite{wikipediawar}. Although this study used a different methodology to my project (since it compared texts in different languages), it provides evidence that sentiment analysis can work on corpora concerning war.
\\\\
There have been a some papers detailing sentiment analysis on transcripts of US Congress debates \cite{rep_dem_one, rep_dem_two}. The results of these studies indicate that determining an MP's stance on the Iraq war from their Parliamentary speeches may be possible. However, these studies determine a politician's political party, which is likely to be more clear-cut than their stance on a particular issue.

\section{Overview of the Project}
\hyperref[prep]{Chapter \ref{prep}} defines the project, then outlines the relevant models and algorithms before discussing implementation decisions. \hyperref[impl]{Chapter \ref{impl}} details the system's development, while \hyperref[eval]{chapter \ref{eval}} assesses the project's success. \hyperref[conc]{Chapter \ref{conc}} summarises the project's accomplishments and implications, commenting on potential further work related to the project.

\chapter{Preparation} \label{prep}


There were three main stages to the project's preparation:
\begin{enumerate}
	\item Defining and planning the project. A project of this scale needs a clear definition of its goals and a well-defined plan designed to achieve these goals. Sections \S\ref{prep-requirements}, \S\ref{prep-changes} \& \S\ref{prep-start} detail this stage.

	\item Learning about the relevant concepts and methods. This was useful as it helped me to make informed decisions about implementation decisions. This required considerable work, as most of the skills and knowledge required to undertake the project are not taught in the Cambridge BA Computer Science course and the parts that are taught are Part II courses. The timescale of the Part II project meant that I had to learn the courses ahead of the lectures. Sections \S\ref{prep-supervised} through \& \S\ref{prep-bow} detail this stage.
	
	\item Specifying the details of the implementation. Sections \S\ref{prep-sweng} \S\ref{prep-tools} detail this stage.
\end{enumerate}

\section{Requirements Analysis} \label{prep-requirements}
The primary goals of this project are to:
\begin{itemize}
	\item Construct a database that comprises British texts on the Iraq war.
	\item Develop a classifier that can determine the stance of the texts in the database.
\end{itemize}
\vspace{5pt}
I will be using the Hansard \cite{hansard} (discussed further in \S\ref{prep-changes}) as the corpus to construct the database from. This allows me to refine the goals as in table \ref{table:tasks}. The tasks in table \ref{table:tasks} are in order of descending priority, due to their dependence on each other.
\begin{table}[]
	\label{table:tasks}
	\centering
	\begin{tabular}{@{}ll@{}}
		\toprule
		\textbf{Task}                                                          & \textbf{Section}          \\ \midrule
		Scrape the relevant data from the Hansard                              & \S\ref{impl-scraping}   \\
		Wrangle the textual data so it is in a more consistent form          & \S\ref{impl-wrangling}  \\
	    Collate the data from the transcripts with voting data           & \S\ref{impl-labelling}  \\
	    Construct a database of the dataset I have created            	   & \S\ref{impl-database}   \\
		Develop a system that can predict the stance of a text on the Iraq war & \S\ref{impl-classifier} \\ \bottomrule
	\end{tabular}
	\caption{Breakdown of the project's core tasks}
\end{table}
\newline
\newline
With any software project, it is necessary to consistently consider the project's requirements and how these will be evaluated. In \S\ref{prep-bayes} I discuss the Na\"{i}ve Bayes Classifier, which I use as a baseline for the classifier and in \S\ref{prep-eval} I consider further aspects of evaluation.

\section{Changes from the Initial Proposal} \label{prep-changes}
In the proposal (see Appendix \ref{sec:proposal}), I wrote about using the dataset produced by Robinson et al. in which they ``evaluated media performance during the 2003 Iraq War'' \cite{iraq_media_study}. As part of their evaluation, they manually annotated the stance of British newspaper articles on the Iraq war. They published the resulting dataset, but it didn't contain the body of the articles - only its headline, author, newspaper and publication date. I consequently investigated resources containing the text of the relevant articles and tried to cross-reference the data from these sources with the manually annotated stance. At the time, many newspapers published different stories online and in print, meaning that I could not rely on these. A few newspapers maintain electronic archives of their printed editions on the internet, however not enough newspapers had such archives. The final resource I looked into was Dow Jones Factiva, a ``global news database'' \cite{factiva}. Upon inspection, this database contained the majority of the articles I needed and it was possible for me to cross-reference the articles in it with the labels annotated by Robinson et al. I initially accessed the dataset through the University's subscription. I therefore (falsely) assumed that this subscription would be sufficient for use in my project, however, I later discovered that an academic licence didn't permit me to use the API or to carry out text-mining. I consequently contacted Dow Jones and was told that the licence I required would cost in excess of \$20,000.
\newline
\newline
After exhausting all other options, I turned my attention to the Hansard archives, which contain transcripts of House of Commons' debates \cite{hansard}. One of the benefits of this dataset is that the texts can be labelled using MPs' voting records.
\newline
\newline
Due to the licensing problems I encountered with Dow Jones Factiva \cite{factiva}, I immediately looked into the licence required to scrape data from the Hansard and found that it is covered by the Open Parliament Licence \cite{open_parliament_licence}. Since the Hansard archives are available under this licence, I was permitted to:
\begin{itemize}
	\item ``copy, publish, distribute and transmit the information''
	\item ``adapt the information''
	\item ``exploit the information commercially and non-commercially, for example, by combining it with other information, or by including it in your own product or application''.
\end{itemize}

\section{Starting Point} \label{prep-start}
For the reasons described in \S\ref{prep-changes}, the actual starting point for this project differs from what I stated in the proposal (see Appendix \ref{sec:proposal}). In \S\ref{intro-related}, I discussed research that is relevant to this project. The project builds on the Hansard \cite{hansard} and Parliamentary voting records to produce a dataset which combines the two. The project also uses various Python libraries, which are specified in \S\ref{prep-tools-libs}.
\section{Introduction to Supervised Learning} \label{prep-supervised} \label{supervised-learning}
This project is a supervised learning project since it uses labelled speeches to determine the class (i.e. pro-war or anti-war) of previously unseen texts. Supervised learning is split into two phases: Learning and predicting.
\newline
\newline
In the learning phase, the system receives inputs of feature vectors and their associated labels. A feature vector of length $k$ is usually denoted by $\mathbf{x}$ where 
\begin{equation}
	\mathbf{x} = (x_1, x_2, ..., x_k) \quad \forall i \in \mathbb{Z}^+ . \forall x_i \in \mathbb{R}.
\end{equation}
A feature vector encodes the information necessary to predict a label. In the context of this project, there is a feature vector for each speech we consider, which contains information about the words in the speech. The label is usually denoted by $y$. The set of values that $y$ can take varies depending on the context of the supervised learning problem. For example, in a regression problem, $y \in \mathbb{R}$. This project concerns binary classification, since we simplify the problem so that we consider all speeches to be either pro-war or anti-war. Because of this, from now on, we will only consider binary classification problems, that is where $y \in \{-1, +1\}$.
\newline
\newline
The supervised learning system creates a function $h$ that takes a feature vector as an input and outputs a label. That is
\begin{equation}
	h(\mathbf{x}) = y.
\end{equation}
This definition allows us to intuitively view each feature vector as a point in k-dimensional space. We consider each point to be either negative ($y = -1$) or positive ($y = +1$). In this analogy, $h$ is a function that determines whether a point is negative or positive, depending on where it is in the k-dimensional space. The more points that $h$ sees, the better its estimation of whether new unseen points are negative or positive.
\newline
\newline
Figure \ref{fig:2dpoints} shows a visualisation of our intuition of feature vectors, where $k = 2$. In this diagram, the supervised learning system learns a function to distinguish the `-' and `+' points. Given a new, previously unseen point, this function would be able to estimate whether it is a `-' or a `+'.
\begin{figure}
	\begin{center}
		\includegraphics[scale=0.6]{figs/2dpoints.png}
	\end{center}
	\caption{A simple 2D plot of four labelled features.}
	\label{fig:2dpoints}
\end{figure}
\section{Introduction to the Na\"{i}ve Bayes Classifier} \label{prep-bayes}

This is one of the simplest classifiers to understand and implement. It uses the assumption that all features are independent of each other:
\begin{equation}
	p(x_i | x_j) = p(x_i) \quad \forall i, j \in \mathbb{Z}^+.
\end{equation}
The classifier is `na\"{i}ve', since this assumption is rarely true. Despite this, the classifier still achieves good performance \cite{ml_book_murphy}.
\newline
\newline
In addition to this assumption, the classifier uses Bayes Theorem:
\begin{equation}
	P(A | B) = \frac{P(B | A)P(A)}{P(B)}.
\end{equation}
The intuition behind the classifier is that given a set of features $\mathbf{x}$, we should assign it to the class that has the highest probability, given the set of features. Using the assumption of conditional independence and Bayes Theorem, we can compute this probability:
\begin{equation}
\begin{aligned}
p(C = y | \mathbf{x}) &= \frac{p(\mathbf{x} | C = y)p(C = y)}{p(\mathbf{x})} \\
&= \frac{p(x_1, \ldots, x_k | C = y)p(C = y)}{p(\mathbf{x})} \\
&= \frac{p(x_1, \ldots, x_k, C = y)}{p(\mathbf{x})} \\
&= \frac{p(x_1 | x_2, \ldots, x_k, C = y)p(x_2, \ldots, x_k, C = y)}{p(\mathbf{x})} \\
& \vdots \\
&= \frac{p(x_1 | x_2, \ldots, x_k, C = y)p(x_2 | x_3 \ldots, x_k, C = y)\cdots p(x_k | C = y)}{p(\mathbf{x})} \\
&= \frac{p(x_1 | C = y)p(x_2 | C = y) \cdots p(x_k | C = y)}{p(\mathbf{x})} \\
&= \frac{\prod_{i = 1}^{k}p(x_i | C = y)}{p(\mathbf{x})}. \\
\end{aligned}
\end{equation}
\\
This shows that we can compute y using:

\begin{equation}
\begin{aligned}
y &= \underset{y}{\operatorname{argmax}}\bigg(\frac{\prod_{i = 1}^{k}p(x_i | C = y)}{p(\mathbf{x})}\bigg) \\
&= \underset{y}{\operatorname{argmax}} \displaystyle \prod_{i = 1}^{k}p(x_i | C = y). \\
\end{aligned}
\end{equation}
\\
We can estimate each $p(x_i | C = y)$ trivially using the training data. Given that in this project I am only considering binary classifiers, where $y \in {-1, +1}$, we can write this as:

\begin{equation}
\text{max} \bigg( \displaystyle \prod_{i = 1}^{k}p(x_i | C = -1), \prod_{i = 1}^{k}p(x_i | C = +1) \bigg). \\
\end{equation}
\\
Due to its simplicity and good performance, I use the na\"{i}ve Bayes classifier as the baseline for this project.
\section{Introduction to Support Vector Machines} \label{prep-svm}

Support vector machines (SVMs) are widely used, state-of-the-art classifiers which were designed for binary classification (although they have since been modified to work for multi-class classification) \cite{ml_book}. Since I'm viewing the task of determining the sentiment of speeches on the Iraq war as a binary classification problem, using an SVM is a natural choice.
\newline
\newline
In contrast to the na\"{i}ve Bayes classifier, the SVM approach to classification is not probabilistic - SVMs are a form of maximum margin classifier. A maximum margin classifier computes a hyperplane of the form
\begin{equation} \label{eq:hyperplane}
	\mathbf{w} \cdot \mathbf{x} + b = 0
\end{equation}
where $\mathbf{w}$ is a normal to the hyperplane. $\mathbf{w}$ and $b$ are determined by the maximisation (\ref{eq:maxdelta}) and $\mathbf{x}$ is a point on the hyperplane. This hyperplane separates the training data so that for all positive examples
\begin{equation}
\mathbf{w} \cdot \mathbf{x} + b \ge 0
\end{equation}
and for all negative examples
\begin{equation}
\mathbf{w} \cdot \mathbf{x} + b < 0.
\end{equation}
The idea of the maximum margin classifier is that it maximises $\delta$, the distance between the hyperplane and the closest examples to it. That is, it computes
\begin{equation} \label{eq:maxdelta}
	\underset{\mathbf{w}, b}{\operatorname{argmax}}(min(\delta)).
\end{equation}
Figure \ref{fig:hyperplane} illustrates this problem in a 2D space.
\begin{figure}
	\begin{center}
		\includegraphics[scale=0.6]{figs/hyperplane.png}
	\end{center}
	\caption{A simple 2D plot of four labelled features and a hyperplane (which is a line in two dimensions) distinguishing the positive and negative points.}
	\label{fig:hyperplane}
\end{figure}
\newline
\newline
To determine how the feature vector $\mathbf{x}_i$ should be  labelled, we simply need to determine which side of the hyperplane it lies on. This gives us the decision function
\begin{equation} \label{eq:initialdecision}
	y_i =
	\begin{cases}
		+1, \quad \mathbf{w} \cdot \mathbf{u} + b \ge 0 \\
		-1, \quad \text{otherwise}
	\end{cases}
\end{equation}
where $\mathbf{u}$ is the feature vector being classified. The support vectors are defined as the training examples that lie closest to the hyperplane. From the equation of the hyperplane, (\ref{eq:hyperplane}), we see that we have the freedom to scale $\mathbf{w}$ and $b$ by a constant factor without changing the hyperplane itself. We can, therefore, define this scaling by imposing the following constraint on all training examples for mathematical convenience:
\begin{equation}
	y_i(\mathbf{w} \cdot \mathbf{x}_i + b) - 1 \ge 0.
\end{equation}
For the support vectors, we then have
\begin{align}
	& y_i(\mathbf{w} \cdot \mathbf{x}_i + b) - 1 = 0 \label{eq:svconstraint}\\
	\implies & \mathbf{w} \cdot \mathbf{x}_i = \frac{1}{y_i} - b \label{eq:wdotx} \\
	\implies & b = \frac{1}{y_i} - \mathbf{w} \cdot \mathbf{x}_i. \label{eq:b}
\end{align}
\\
We now need to compute the width of the margin so we can form an expression to maximise it. Figure \ref{fig:hyperplane} gives us some intuition as to how we can achieve this. Since $\mathbf{w}$ is perpendicular to the hyperplane, $\frac{\mathbf{w}}{||\mathbf{w}||}$ must be a unit normal to the hyperplane. We can then use a positively labelled support vector, $\mathbf{x}_+$ and a negatively labelled support vector, $\mathbf{x}_-$ to get an expression for the margin width:
\begin{equation}
\begin{aligned}
	\text{Margin width} & = \frac{\mathbf{w}}{||\mathbf{w}||} \cdot (\mathbf{x}_+ - \mathbf{x}_-) \\
	& = \frac{\mathbf{w} \cdot \mathbf{x}_+ - \mathbf{w} \cdot \mathbf{x}_-}{||\mathbf{w}||}.
\end{aligned}
\end{equation}
We can now substitute in the result from (\ref{eq:wdotx}) to give
\begin{equation} \label{eq:width}
\begin{aligned}
\text{Margin width} & = \frac{\big(\frac{1}{y_+} - b\big) - \big(\frac{1}{y_-} - b\big)}{||\mathbf{w}||} \\
& = \frac{\frac{1}{y_+} - \frac{1}{y_-}}{||\mathbf{w}||} \\
& = \frac{1 + 1}{||\mathbf{w}||} \\
& = \frac{2}{||\mathbf{w}||} \\
\end{aligned}
\end{equation}
Our goal is to maximise the width given by (\ref{eq:width}). For mathematical convenience, we can instead solve the equivalent problem of minimising $\frac{1}{2}||\mathbf{w}||^2$. This optimisation is subject to the constraints in (\ref{eq:svconstraint}). In order to solve this constrained optimisation problem, we can use Lagrange multipliers
\begin{equation}
	\boldsymbol{\alpha} = (\alpha_1, \alpha_2, \ldots, \alpha_n)
\end{equation}
where $n$ is the number of training examples. This results in the Lagrangian
\begin{equation} \label{eq:initialL}
	L = \frac{1}{2}||\mathbf{w}||^2 - \sum_{i = 1}^{n} \alpha_i (y_i(\mathbf{w} \cdot \mathbf{x}_i + b) - 1). 
\end{equation}
Our task is now to solve the unconstrained maximisation problem
\begin{equation} \label{eq:lagrangeoptimisation}
	(\mathbf{w}_{opt}, b_{opt}) = \underset{\mathbf{w}, b}{\operatorname{argmax}}(L)
\end{equation}
Using the Karush-Kuhn-Tucker conditions, we can show that $\alpha_i = 0$ for all feature vectors that are not support vectors \cite{ml_book}. This results in fast computation and means that after training, we only need to store the support vectors. Therefore, from now on, we will sum over $\mathcal{S}$, the set of indices corresponding to the support vectors.
\newline
\newline
In order to solve the optimisation problem defined in (\ref{eq:lagrangeoptimisation}), we must find the partial derivative of L with respect to both $\mathbf{w}$ and $b$, setting the resulting expressions to 0 (since we want to vary $\mathbf{w}$ and $b$ in order to find the maximum L).
\begin{align}
	\frac{\partial L}{\partial \mathbf{w}} & = \mathbf{w} - \sum_{i \in \mathcal{S}} \alpha_i y_i \mathbf{x}_i = 0 \\
	\implies \mathbf{w} & = \sum_{i \in \mathcal{S}} \alpha_i y_i \mathbf{x}_i \label{eq:wsum} \\
	\frac{\partial L}{\partial b} & = \sum_{i \in \mathcal{S}} \alpha_i y_i = 0 \label{eq:db}
\end{align}
We can now substitute (\ref{eq:wsum}) into (\ref{eq:initialL}) to obtain a new expression for $L$ (and simplify using (\ref{eq:db})) as follows:
\begin{equation} \label{eq:separableL}
\begin{aligned}
	L & = \frac{1}{2}(\sum_{i \in \mathcal{S}} \alpha_i y_i \mathbf{x}_i)(\sum_{j \in \mathcal{S}} \alpha_j y_j \mathbf{x}_j) - \sum_{i \in \mathcal{S}} \alpha_i y_i(\sum_{j \in \mathcal{S}} \alpha_j y_j \mathbf{x}_j) \cdot \mathbf{x}_i - b\sum_{i \in \mathcal{S}} \alpha_i y_i + \sum_{i \in \mathcal{S}} \alpha_i \\
	& = \frac{1}{2}(\sum_{i \in \mathcal{S}} \alpha_i y_i \mathbf{x}_i)(\sum_{j \in \mathcal{S}} \alpha_j y_j \mathbf{x}_j) - (\sum_{i \in \mathcal{S}} \alpha_i y_i \mathbf{x}_i)(\sum_{j \in \mathcal{S}} \alpha_j y_j \mathbf{x}_j) + \sum_{i \in \mathcal{S}} \alpha_i \\
	& = \sum_{i \in \mathcal{S}} \alpha_i - \frac{1}{2}(\sum_{i \in \mathcal{S}} \alpha_i y_i \mathbf{x}_i)(\sum_{j \in \mathcal{S}} \alpha_j y_j \mathbf{x}_j) \\
	& = \sum_{i \in \mathcal{S}} \alpha_i - \frac{1}{2}\sum_{i \in \mathcal{S}} \sum_{j \in \mathcal{S}} \alpha_i \alpha_j y_i y_j (\mathbf{x}_i \cdot \mathbf{x}_j). \\
\end{aligned}
\end{equation}
We now need to find the values $\boldsymbol{\alpha}$ which maximise L:
\begin{equation} \label{eq:maximisation}
	\boldsymbol{\alpha}_{opt} = \underset{\boldsymbol{\alpha}}{\operatorname{argmax}}(L).
\end{equation}
We can then find these values numerically. Further to this, it can be shown that the space of $L$ is convex, so we will not find a local maximum. This is a significant advantage of using SVMs over neural networks.
\newline
\newline
We can then find $\mathbf{w}_{opt}$ by substituting $\boldsymbol{\alpha}_{opt}$ into (\ref{eq:wsum}) and using the support vectors and their labels. From this, we can find $b_{opt}$ using (\ref{eq:b}) and substituting in $\mathbf{w}_{opt}$ along with any support vector and its label. Substituting our values for $\boldsymbol{\alpha}_{opt}$ and $b_{opt}$ into the initial decision rule we obtain a new decision rule:
\begin{equation} \label{eq:separabledecision}
	y_i =
	\begin{cases}
		+1, \quad \displaystyle \sum_{i \in \mathcal{S}} y_i(\boldsymbol{\alpha}_{opt})_i (\mathbf{x}_i \cdot \mathbf{u}) + b \ge 0 \\
		-1, \quad \text{otherwise}.
	\end{cases}
\end{equation}
Thus far, we have been working under the assumption that our data is linearly separable. In practice, this is rarely the case and for this project due to the inherent noise in our data (described in \S \ref{impl-labelling}), this assumption is very unlikely to hold. Figure \ref{fig:inseparable} illustrates a simple example for which the data are not linearly separable.
\begin{figure}
	\begin{center}
		\includegraphics[scale=0.6]{figs/inseparable.png}
	\end{center}
	\caption{A simple 2D plot of four linearly inseparable labelled features.}
	\label{fig:inseparable}
\end{figure}
\newline
\newline
In order to fix this problem, we can use a transformation, $\phi$, to map our feature vectors into a space in which our data is more easily separable. Applying this transformation to (\ref{eq:separableL}) gives us a new $L$:
\begin{equation} \label{eq:phiL}
	L = \sum_{i \in \mathcal{S}} \alpha_i - \frac{1}{2}\sum_{i \in \mathcal{S}} \sum_{j \in \mathcal{S}} \alpha_i \alpha_j y_i y_j (\phi (\mathbf{x}_i) \cdot \phi (\mathbf{x}_j)).
\end{equation}
We maximise this as before, finding a new $\boldsymbol{\alpha}_{opt}$ from (\ref{eq:maximisation}) and then using those values to find $b_{opt}$. We can then use these values of $\boldsymbol{\alpha}_{opt}$ and $b_{opt}$ along with the transformation $\phi$ to obtain another decision rule:
\begin{equation} \label{eq:phidecision}
	y_i =
	\begin{cases}
		+1, \quad \displaystyle \sum_{i \in \mathcal{S}} y_i(\boldsymbol{\alpha}_{opt})_i (\phi (\mathbf{x}_i) \cdot \phi (\mathbf{u})) + b \ge 0 \\
		-1, \quad \text{otherwise}.
	\end{cases}
\end{equation}
We are yet to define $\phi$, but if we consider the contexts in which it is used, we see that it is always in the form $\phi (\mathbf{x}) \cdot \phi (\mathbf{x}')$. Therefore, rather than define $\phi$ itself, we define a kernel function
\begin{equation}
	k(\mathbf{x}, \mathbf{x}') = \phi (\mathbf{x}) \cdot \phi (\mathbf{x}').
\end{equation}
From this, we can rewrite (\ref{eq:phiL}) and (\ref{eq:phidecision}) as:
\begin{equation}
	L = \sum_{i \in \mathcal{S}} \alpha_i - \frac{1}{2}\sum_{i \in \mathcal{S}} \sum_{j \in \mathcal{S}} \alpha_i \alpha_j y_i y_j k(\mathbf{x}_i, \mathbf{x}_j) \\
\end{equation}
\begin{equation}
	y_i =
	\begin{cases}
		+1, \quad \displaystyle \sum_{i \in \mathcal{S}} y_i(\boldsymbol{\alpha}_{opt})_i k(\mathbf{x}_i, \mathbf{u}) + b \ge 0 \\
		-1, \quad \text{otherwise}.
	\end{cases}
\end{equation}
The choice of kernel function can have a significant effect on the performance of an SVM. In order to ensure good results, I train the SVM using different kernel functions, so the classifier learns which kernel function works best for the data. The three kernel functions I consider are:
\begin{align}
	k(\mathbf{x}, \mathbf{x}') & = \mathbf{x} \cdot \mathbf{x}'& \label{eq:linearkernel} \\
	k(\mathbf{x}, \mathbf{x}') & = (\gamma (\mathbf{x} \cdot \mathbf{x}') + r)^d \quad & \gamma \in \mathbb{R}_{> 0} . r \in \mathbb{R}_{\ge 0} . d \in \mathbb{N}_{> 0} \label{eq:polykernel} \\
	k(\mathbf{x}, \mathbf{x}') & = e^{-\gamma ||\mathbf{x} - \mathbf{x}'||^2} \quad & \gamma \in \mathbb{R}_{> 0} \label{eq:rbfkernel}
\end{align}
From now on, I will refer to (\ref{eq:linearkernel}), (\ref{eq:polykernel}) and (\ref{eq:rbfkernel}) as the linear kernel, the polynomial kernel and the radial basis function (RBF) kernel respectively. Using the linear kernel is equivalent to our SVM before we introduced the transformation $\phi$. This shows that even with kernel functions, our data may not be linearly separable. It can also be shown that the polynomial kernel does not necessarily transform the data so that into a space in which it is linearly separable, but the RBF kernel can always map the feature vectors to a space where they are linearly separable. This means that for the linear and polynomial kernels, we may not be able to produce a classifier with the given constraints and for the RBF kernel, the SVM is very susceptible to overfitting. Both of these problems can be solved by introducing soft-margins to our SVM. This means that we will allow the SVM to incorrectly classify some of the training examples. In order to do this, we introduce a parameter $C$ which trades off correct classification of training examples with a greater margin width. A greater margin width results in a smoother function, so means that the SVM is less likely to overfit. The higher the value of $C$, the more training examples the SVM will fit correctly. $C$ is a hyperparameter - that is a parameter whose value is fixed before the classifier is trained. All of the hyperparameters for each of the kernels we are considering are given in table \ref{table:hyperparameters}. The hyperparameter choice significantly effects the performance of the SVM, so we need an algorithm for choosing them. This is discussed further in both \S\ref{cross-validation} \& \S\ref{impl-classifier}.

\begin{table}[]
	\centering
	\label{table:hyperparameters}
	\begin{tabular}{ll}
		\textbf{Kernel}       & \textbf{Hyperparameters} \\ \hline
		Linear                & $C$                      \\
		Polynomial            & $C$, $\gamma$, $r$, $d$  \\
		Radial basis function & $C$, $\gamma$             
	\end{tabular}
	\caption{The hyperparameters of various kernels}
\end{table}

\section{Introduction to Evaluating Supervised Learning Systems} \label{prep-eval}
\subsection{Train/Test Split} \label{train-test-split}
In \S\ref{supervised-learning}, we saw how a supervised learning system is trained on one set of data (the training set), then this trained model is used to predict the labels of previously unseen data (the testing set). In order to evaluate a system, we require the actual data labels, so we can assess the predictions. If we have training examples in the testing set results, we will not obtain a reliable measure of the system's performance, as it would unfairly reward overfitting. In order to overcome this, we must split our labelled data before we start developing a model. Using 90\% for training and 10\% for testing is the most common way to split the labelled data.
\subsection{Overfitting} \label{prep-overfitting}

Overfitting is a common problem in supervised learning problems. It occurs when a classifier learns to get a very high performance on the training set but does not generalise well to unseen data, meaning that the classifier achieves poor results on the test data. Figure \ref{fig:overfitting} illustrates a simple example of a (two-dimensional) hyperplane that overfits the given data.
\begin{figure}
	\begin{center}
		\includegraphics[scale=0.6]{figs/overfitting.png}
	\end{center}
	\caption{An example of the an overfitting function.}
	\label{fig:overfitting}
\end{figure}
\subsection{Cross-Validation} \label{cross-validation}
Cross-validation is a way to mitigate overfitting. To use this scheme, we split the training set into $k$ disjoint folds. Doing this means that we can iterate the process of training and evaluate without using the testing set. This is done by training on $k-1$ of the folds, then evaluating the system on the left out fold. This is repeated $k$ times, so each fold is used for evaluation once. Averaging over all the folds gives a reliable evaluation metric. We can use this method to determine the system's hyperparameters and any other settings that need to be determined before training. To do this, we repeat the method described for different combinations of hyperparameters and settings, selecting the combination whose average evaluation metric is greatest. In this project, I use stratified cross-validation - for this scheme, when splitting the data into folds, I ensure that each fold has a roughly even split of positive and negative examples.
\subsection{Evaluation Metrics} \label{evaluation-metrics}
Thus far, we have only spoken abstractly about evaluation metrics. There are various options and choice of the measure should be context-specific. The bases of the definitions used in most metrics are defined in the table \ref{table:true-positives}.
\\
\begin{table}[]
	\centering
	\begin{tabular}{llll}
		&                                                                    & \multicolumn{2}{c}{Prediction} \\
		& \multicolumn{1}{l|}{}                                              & \textbf{Positive}  & \textbf{Negative}  \\ \cline{2-4} 
		\multirow{2}{*}{Actual Value}&\multicolumn{1}{l|}{\textbf{Positive}} & True Positive      & False Negative     \\
		& \multicolumn{1}{l|}{\textbf{Negative}}                             & False Positive     & True Negative     
	\end{tabular}
	\caption{Defining true positives, true negatives, false positives and false negatives}
	\label{table:true-positives}
\end{table}
\\
From this, we can now define the following quantities:
\begin{align}
	TP & = \text{Number of True Positives} \\
	FN & = \text{Number of False Negatives} \\
	FP & = \text{Number of False Positives} \\
	TN & = \text{Number of True Negatives}
\end{align}
Accuracy is the simplest evaluation metric. We define this as:
\begin{equation}
	\text{Accuracy} = \frac{TP + TN}{TP + FN + FP + TN}
\end{equation}
If we have an unbalanced dataset (which is the case in this project), then accuracy is not a good evaluation metric. To illustrate this, consider the case where 1\% of our data is positive and 99\% of our data is negative. If we had a classifier that always predicted that an example was negative, its accuracy would be 99\%.
\newline
\newline
We now define two further measures:
\begin{align}
	\text{Precision} & = \frac{TP}{TP + FP} \\
	\text{Recall} & = \frac{TP}{TP + FN}
\end{align}
A good evaluation metric for an unbalanced dataset will incorporate some trade-off between precision and recall. Such a metric may give greater weighting to precision for a precision-critical task or greater weighting to recall for a recall-critical task. Since this project is neither precision-critical nor recall-critical, I use the F1 score as the evaluation metric, as it is defined as the harmonic mean of precision and recall:
\begin{equation}
	\text{F1} = \frac{2 \times \text{precision} \times \text{recall}}{\text{precision} + \text{recall}}.
\end{equation}
\section{Introduction to the Bag-of-Words Model} \label{prep-bow}

In mathematics, a bag is a synonym for a multiset - an abstract data type which is like a set but differs in that it can contain duplicates. In this project, I will represent MPs' speeches using a bag-of-words model, meaning that the representation ignores the order of words. Despite its simplicity, the bag-of-words model is used successfully in a range of sentiment classification applications, as it can effectively capture the discourse of a text \cite{nlp_book}.
\newline
\newline
To illustrate a bag-of-words model using an example, I will use a quote from a House of Commons debate on 18th March 2003:
\begin{center}
	\textit{``The best way to avoid war is to work through the United Nations.''}
	\newline
	 - Bill Tynan (Labour Party)
\end{center}
In a simple bag-of-words implementation, where case and punctuation are ignored, this sentence would be stored as:
\begin{align*}
	\{\text{the}&: 2,\\ \text{to}&: 2,\\ \text{best}&: 1,\\ \text{way}&: 1,\\ \text{avoid}&: 1,\\ \text{war}&: 1,\\ \text{is}&: 1,\\ \text{work}&: 1,\\ \text{through}&: 1,\\ \text{united}&: 1,\\ \text{nations}&: 1\}.
\end{align*}
It is important to note that the order of the elements above is not relevant.

\section{Software Engineering Techniques} \label{prep-sweng}
	
Before implementing the classifier, I couldn't be sure that it would be possible to develop a system to classify MP's speeches, due to the lack of similar work. Because of this, it was important to develop the first classifier as quickly as possible and then change the project requirements at that point if the data couldn't be classified. This strategy lent itself to agile software development \cite{agile}, so this is what I used. I used each of the tasks in table \ref{table:tasks} to establish the project's first sprints, later defining further sprints as extensions.
\newline
\newline
Throughout development, I used a linter to maintain a high standard of code, with consistent documentation. I also used revision control and designed programs to be as modular as possible, which helped with testing and debugging.
\newline
\newline
Development of machine learning systems requires further good practices to be adopted, to avoid overfitting. To this end, as soon as I had constructed the database, I split the data into a training set and a testing set. Throughout the implementation, I solely used the training set, carrying out cross-validation across it to test code then using the testing set for evaluation. Further to this, before classifying MPs' speeches, I implemented a classifier for spam emails (see \S\ref{eval-spam}) and then adapted this classifier for the purposes of this project, which ensured that the classifier generalised well.


\section{Choice of Tools} \label{prep-tools}

\subsection{Programming Language}

I decided to develop the project in Python for various reasons:
\begin{itemize}
	\item There are many good natural language processing and supervised learning libraries available for Python.
	\item Python is well-suited to agile development.
	\item Python has a lot of community support available.
	\item I have previously used Python for large software projects (working in industry).
\end{itemize}
I used Python 3.6 since it was the most recent stable version of Python when I started implementation.

\subsection{Database} \label{prep-database}

Due to the structure of the data, it made sense to use a relational database. After considering various options, I opted to use SQLite due to its low set-up overheads (which is useful for agile development), its stability and its compatibility with Python. I justify this decision further in \S\ref{impl-database}.

\subsection{Libraries} \label{prep-tools-libs}

I used BeautifulSoup to parse the Hansard's HTML and the \texttt{.ems} files from the spam email dataset. Although lxml is a faster parser \cite{parsing}, BeautifulSoup copes better with `broken' HTML. Before implementation, I found various inconsistencies with the Hansard's HTML so BeautifulSoup was a better choice (especially since there were no significant time constraints on parsing). The Natural Language Toolkit is a widely used library with many NLP functions, so I used this for various things. I used functions from scikit-learn when implementing the classifier. To perform fast mathematical computations, I used both NumPy and SciPy. In order to perform fast HTTP requests, I used the Requests library. In addition to the libraries mentioned, I also used different modules from the Python standard library.

\subsection{Development Environment}

I predominantly developed the project using my own laptop running the Windows 10 operating system. I used Visual Studio Code as the source code editor, with Python and PyLint extensions. One advantage of using Visual Studio Code is the easy integration with git, which I used for revision control of both the Python source code and this dissertation's \LaTeX \space source code. Since I used git for revision control, I used GitHub to backup the source code. I also synced all of my dissertation files to Google Drive and periodically backed them up to an external disk.

\subsection{Table of Software Used}
\begin{table}[H]
	\centering
	\label{table:software}
	\begin{tabular}{ll}
		\textbf{Software}        & \textbf{Version} \\ \hline
		BeautifulSoup            & 4.6.0            \\
		git                      & 2.8.1.windows.1  \\
		Matplotlib               & 2.1.2            \\
		Natural Language Toolkit & 3.2.5            \\
		NumPy                    & 1.13.3           \\
		Pandas                   & 0.22.0           \\
		PyLint                   & 1.7.4            \\
		Python                   & 3.6.3            \\
		Requests                 & 2.18.4           \\
		Scikit-Learn             & 0.0              \\
		SciPy                    & 1.0.0            \\
		SQLite                   & 3.10.1           \\
		Visual Studio Code       & 1.20.1           \\
		Windows 10               & Home             \\
	\end{tabular}
	\caption{The versions of the software used in this project}
\end{table}

\section{Summary}

In this chapter, I have defined the project and the steps I took before starting its development. These steps involved planning the project and how its success will be measured, thereby allowing smoother implementation and evaluation.

\chapter{Implementation} \label{impl}

This chapter discusses the implementation of the project's components. \S\ref{impl-architecture} provides an overview of the system's implementation. \S\ref{impl-structures} talks about some of the design decisions related to specific data structures. \S\ref{impl-acquisition} gives details about compiling the project's dataset and \S\ref{impl-classifier} discusses the implementation of the classifier.


\section{System Architecture} \label{impl-architecture}

Through the requirements analysis in \S\ref{prep-requirements}, I established the project's core tasks and their dependencies (see table \ref{table:tasks}). From this, I was able to create a project timetable, broken down into sprints. After completing the essential components of the project, I improved the classifier by implementing a series of extensions. The list below gives each of the sprints I carried out:
\begin{enumerate}
	\item Scraped the relevant data from the Hansard.
	\item Wrangled the textual data into a more consistent form.
	\item Collated the data from the transcripts with voting data.
	\item Constructed a database of the dataset I have created.
	\item Parsed the spam email dataset.
	\item Developed an SVM classifier to detect spam email.
	\item Developed a na\"{i}ve Bayes classifier to predict the stance of MPs' speeches.
	\item Used the spam email classifier as a basis to develop an SVM classifier to predict the stance of MPs' speeches on the Iraq war.
	\item Extension: Implemented stopword removal, stemming, n-grams and number-grouping.
	\item Extension: Implemented an algorithm to allow the SVM classifiers to learn the best combination of settings and hyperparameters.
	\item Extension: Optimised the SVM classifiers by carrying out singular value decomposition on the feature vectors.
\end{enumerate}
\vspace{5pt}
In addition to these sprints, I also developed an entailment system, using the questions associated with Parliamentary votes, along with the features I use for the sentiment analysis. Since this sprint was tangential to the project and didn't produce very good results, I do not mention it further in this dissertation.
\\\\
As this project is very data intensive, I defined how data would flow before commencing implementation. Figure \ref{fig:dataflow} shows the movement of data between modules and data sources (including local files). Data moves in the direction of the arrows. This diagram shows the project's dependency on the Hansard, data.parliament.uk and the spam email dataset. I, therefore, ensured that I had backed-up copies of the data from these sources at the earliest possible stage in the project, to mitigate any issues if the servers hosting these datasets went down at any point during the project.
\newline
\begin{figure}
	\includegraphics[width=\linewidth]{figs/dataflow3.png}
	\caption{Data flow in the project.}
	\label{fig:dataflow}
\end{figure}
\\
Deciding upon this data flow, allowed me to develop a finer project structure. Figure \ref{fig:dependencies} shows the internal dependencies of the modules in the project. An arrow from A to B indicates that A is dependent on B. Note that I designed the system in a way that avoids cyclic dependencies and maximises code-sharing between modules.

\begin{figure}
	\includegraphics[width=\linewidth]{figs/internaldependencies.png}
	\caption{Internal dependencies within the project.}
	\label{fig:dependencies}
\end{figure}

\section{Data Structures} \label{impl-structures}

The size of the project means that I used a variety of data structures. In this section, I discuss the choice and use of a few of these.

\subsection{NumPy Arrays}

I used various parts of the NumPy library throughout the project. For example, I frequently used NumPy arrays in situations where a simple Python list would have sufficed. For this project, NumPy arrays had various advantages over Python lists:
\begin{itemize}
	\item Operations on them are faster - they were specifically designed with performance in mind, which Python was not \cite{numpyspeed}.
	\item They are more space efficient - their low-level implementation means that they use contiguous blocks of memory, rather than a series of pointers \cite{numpymanual}.
	\item They support matrix operations with no additional work.
\end{itemize}
\vspace{5pt}
These benefits were particularly clear given the large amount of data I was processing. Since the features were the largest data structures I used for numerical computations, they made the advantages of using NumPy arrays clearest.

\subsection{Sets}

The set data structure is built-in to Python and is implemented using a hash table \cite{pythonsetdocs}, meaning that lookup operations are performed in constant time. This, in turn, speeds up the implementation of other set functions, such as union, particularly for data which significantly overlap. For this reason, I used the Python set data type wherever it would improve the efficiency of a function.

\subsection{Counters} 

The Counter class is part of the collections module, which is part of the Python standard library, meaning that there is ample support available for its use. The data structure is optimised for counting hashable objects \cite{pythoncollectionsdocs} - it is essentially a multi-set designed to count the number of each element. This makes it perfect for representing a bag-of-words, so it was useful in this project. As discussed in \S\ref{impl-reduction}, it was necessary to reduce the size of the features used for classification, which could easily be done in $\mathcal{O}(k)$ time (where $k$ is the number of dimensions on the reduced features) using Counter's \texttt{most\_common()} method.

\subsection{Sparse Matrices}

The nature of bag-of-words features means that they are primarily populated by zeros. This means that although the dimensions of the feature matrices (matrices where each row is a set of features for a particular example) are very large, they can be compressed into `sparse matrices', making them more space efficient. Matrix multiplications are also sped up since there are algorithms specifically designed for sparse matrices. Sparse matrices were most useful in dimensionality reduction, as they dramatically decreased the time taken for singular value decomposition. Listing \ref{lst:reduce_features} gives the code for this feature reduction using sparse matrices.

\section{Data Acquisition} \label{impl-acquisition}
\subsection{Scraping Data} \label{impl-scraping}

This section discusses how I acquired the relevant data. The difficulty of this sprint was that the transcript data that I required was not available in an API, so I had to scrape the data from the Hansard's inconsistently (and often incorrectly) structured webpages. This meant that I had to develop defensive code to handle many corner cases. As discussed in \S\ref{prep-tools-libs}, I opted to use BeautifulSoup to parse the \texttt{.html} files.
\\\\
Initially, I developed a program that could traverse the Hansard to find the pages containing relevant debates. In order to do this, I iterated over the dates in the range I was considering\footnote{11/09/2001 to 18/03/2003 (inclusive)}, parsing the Hansard's webpage for each of these days to find links to relevant debates on those days. Figure \ref{fig:incorrecthansard} is one of many examples of incorrectly structured data in the Hansard - it suggests that all the debates on that day were prayers. Due to the Hansard's deficiencies, I had to exhaustively search through each House of Commons sitting, which increased the running time of this part of the program. Listing \ref{lst:add_day} gives the high-level code used to traverse the Hansard archives. The Hansard's inconsistencies required me to program defensively, by using extensive exception-handling.
\newline

\begin{figure}
	\includegraphics[width=\linewidth]{figs/incorrecthansard.png}
	\caption{A screenshot of incorrect structure in the Hansard.}
	\label{fig:incorrecthansard}
\end{figure}

\pylisting{High-level code used to scrape all the debates from a given day.}{add_day}

\subsection{Wrangling Data} \label{impl-wrangling}
When parsing the debates and quotes, I faced similar difficulties to parsing the Hansard's pages for each day, meaning that I again had to extensively use exception-handling. To illustrate some of the difficulties of using the Hansard to generate text that is suitable for classification, I have given figure \ref{fig:hansardquote} which is a screenshot of a quote in the Hansard, along with listing \ref{lst:quote}, which is the corresponding HTML paragraph tag. I have trimmed the HTML slightly, so it is easier to read. After reading through the HTML for a representative sample of debates in the Hansard, I wrote some functions to extract the relevant text from the Hansard's HTML, including the method given in listing \ref{lst:get_paragraph_text}, which uses a series of regular expressions to make textual replacements.

\begin{figure}
	\includegraphics[width=\linewidth]{figs/idsquote.png}
	\caption{A screenshot of a quote in the Hansard.}
	\label{fig:hansardquote}
\end{figure}

\htmllisting{HTML paragraph tag of the quote in Figure \ref{fig:hansardquote}.}{quote}

\pylisting{Python code using regular expressions to clean the text from a paragraph tag.}{get_paragraph_text}
\subsection{Labelling Data} \label{impl-labelling}

One of my primary reasons for using the Hansard as the corpus for this project was the fact that I could label the data automatically, using MPs' voting records. This, in turn, meant that the project could use supervised learning rather than unsupervised learning or semi-supervised learning. It was relatively trivial to retrieve voting data - I used the House of Commons Divisions API from data.parliament.uk. One downside to labelling the data in this way is that it introduces noise into the data, as MPs do not vote consistently with their own views due to party whips \cite{whips}, resulting in some mislabelled data. This noise can be accounted for by good selection of hyperparameters.
\\\\
The main difficulty in labelling the data was matching the voting data with the transcript data, due to the fact that MPs' names varied greatly over time. For example, some MPs adopted married names during their time in Parliament and some had titles that were intermittently used to refer to them. Michael Kerr provides a good illustration of the multitude of names of an individual - he is referred to as 13th Marquess of Lothian in data.parliament.uk, the Earl of Ancram in the Hansard and is also known as Baron Kerr of Monteviot. Kerr was one of 36 MPs whose name differed enough between the two data sets that I couldn't algorithmically match their speeches with their voting record. These MPs were mostly unmatchable, due to maiden names, but I manually entered rules to handle them. I matched the other 619 MPs using the function given in listing \ref{lst:match_full_name}, which tries to match the given speaker with one of the MPs in the data.parliament.uk dataset, by removing any honorary titles from the speaker's name (since the data.parliament.uk dataset doesn't give these titles), then finding the most similar\footnote{Similarity is calculated using the Levenshtein distance, which is defined as the number of deletions, insertions, or substitutions required to convert one string into another.} name in the voting record dataset. If this name is sufficiently similar, the speaker is matched. If it is not, a similar function is called, which uses different data in the data.parliament.uk dataset to match the speaker. If this function cannot find a good match either, it raises a \texttt{MatchException}, which is unhandled by the code in listing \ref{lst:match_full_name}, meaning that \texttt{match\_full\_name()} also raises this exception. The code that handles these exceptions creates a file of any unmatched MPs. I manually looked up these MPs to find alternative names for them. In order to minimise run-time, I kept a match list, so that once a match was found for a given MP, the same match could be found in constant time, since I used a Python dictionary (which is implemented using a hash table \cite{pythonfaqs}).

\pylisting{Function that matches an MP's name in the Hansard }{match_full_name}

\subsection{Database} \label{impl-database}

Since the database would only be used locally (so there would only ever be one copy) and I planned on accessing it serially, I could ensure that my database would satisfy the ACID properties (Atomicity, Consistency, Isolation \& Durability) by using a relational database. As described in \S\ref{prep-database}, I opted to use SQLite due to its low overheads - it is referred to as a zero-configuration database \cite{sqlitezeroconfig}. The benefits of this are two-fold; it reduces development time and running time since there is no separate server process and therefore no overhead from message passing to and from the database \cite{sqliteserverless}. Further to this, the authors of SQLite released the code under a licence that allows anyone to ``copy, modify, publish, use, compile, sell, or distribute'' SQLite \cite{sqlitelicence}, which is sufficient for this project.
\\\\
When designing the database schema, I ensured that the database would be in third normal form, meaning that the data would preserve referential integrity and minimise the potential for data duplication. Figure \ref{fig:erdiagram} shows the entity-relationship diagram for the database schema I adopted. Due to the limited type system in SQLite \cite{sqlitetypes}, all fields are of type `TEXT'. This is not a problem, because SQLite has built-in functions to handle data as if they were of different types - for example, the DEBATE and DIVISION entities had DATE attributes, which were represented as text in the database file but I could handle them easily as if they were of a type `DATE'.
\\
\begin{figure}
	\includegraphics[width=\linewidth]{figs/erd.png}
	\caption{The entity-relationship (ER) diagram of the system's database.}
	\label{fig:erdiagram}
\end{figure}
\\
When working with the database, it was useful to be able to manually check the data and therefore the functioning of any program using the database. To do this, I used DB Browser for SQLite, which is a lightweight GUI interface that allowed me to browse the data. This was particularly useful for `evaluating' the data acquisition since there is no formal way for me to automatically check the data in the database. Manually sampling the data using DB Browser for SQLite and cross-referencing it with the original data sources was one of the ways which I evaluated the data acquisition.
\\\\
To maintain good software engineering practices, I only accessed the database through the database class (as shown in Figure \ref{fig:dataflow}). Through doing this and keeping the class's member variables private, I only allowed database accesses through the class's public methods, thereby enforcing encapsulation and information hiding. This meant that I had to implement any database queries I used as functions in the database class. Since database accesses are expensive, I limited them as much as possible, in part by querying the database and storing results at the beginning of the program's execution, then accessing values from variables when they were needed. Listing \ref{lst:get_aye_members_from_term} is an example of my implementation of a database query.
\\\\
\pylisting{Function that performs a query on the database to get the ids of all the MPs who voted `aye' in a given vote and spoke in a debate whose title contains a given term.}{get_aye_members_from_term}

\section{Classifier} \label{impl-classifier}

\subsection{Feature Generation}

Once the database was formed, the first step to develop the classifier was determining which debates to use for the classifier. To do this, I wrote a function to query the database for any debates with titles containing one or more of a given set of terms. I then parsed all the speeches in these debates into lists of words, labelling them according to the MP who made the speech. This parsing was altered depending on the applied optimisations - I discuss some of these in \S\ref{impl-optimisations}. The word lists were then used to create bag-of-words features, in which each element corresponded to an n-gram. This process omitted any terms that were used less than 10 times in the training set. These bag-of-words features contain lots of zeros and have high dimensionality, so they can then be reduced to smaller features using singular value decomposition, as discussed in \S\ref{impl-reduction}.

\subsection{Avoiding Overfitting} \label{impl-overfitting}

I took appropriate measures to avoid overfitting. Firstly, before implementing the program to determine the stance of MPs' speeches, I wrote a classifier to determine whether a given email is spam. I developed this classifier using a bag-of-words representation and a support vector machine, as this is how I planned to write the project's main classifier. Since both tasks are supervised binary classification sentiment analysis problems, I could adapt the spam email classifier, so it determined the sentiment of MPs' speeches. Using this development procedure meant that I avoided making the classifier overly specific. It also provided a further way to evaluate the project, as I discuss in \S\ref{eval-spam}. In addition to this, during development I only ever used the training data, comparing different implementations using averages of F1 scores across cross-validation folds. This meant that there was no scope for developing a classifier that was specifically tailored to produce good results on the testing set.

\subsection{Scikit-learn}

I used the Scikit-learn library to implement the classifier, as it provided a good implementation for both a support vector machine and a na\"{i}ve Bayes classifier, which both have significant documentation \cite{nbcdocs, svmdocs} and community support. Scikit-learn is built on other libraries which improves its performance. For example, uses Cython, which is supports C code within Python programs \cite{cythondocs}. In their evaluation of machine learning libraries in Python, Pedregosa et al. showed that the Scikit-learn support vector machine was the faster than any other implementation they tested \cite{skspeed}. A further benefit of Scikit-learn is that its na\"{i}ve Bayes classifier, which used similar syntax to the support vector machine, so it was easy to establish a baseline F1 score.

\subsection{Optimisations} \label{impl-optimisations}

\subsubsection{Stemming} \label{impl-stem}

Lemmatisation is the process of using morphological processing to reduce a word to its dictionary form (lemma). Unfortunately, lemmatisation is expensive, so stemming is often used as an approximation. Stemming is the process of reducing a word to its stem, using a collection of rules that adapt or remove a word's suffix. Stemming algorithms use heuristics to find a word's stem. In the bag-of-words representation used in this project, stemming means that words with the same stem are counted in the same feature. This is useful, as it is probable that words with the same stem will have similar semantics. Unfortunately, stemming does not perform as well as lemmatisation. For example, a lemmatiser might map `are', `be' and `is' to the lemma `be', since they have the same meaning, whereas the Porter stemmer would leave each of these words unchanged. A further example of where stemming does not have the desired effect is seen when stemming `author' and `authority' - using the Porter stemmer, both words map to `author', despite their different meanings. I implemented stemming using the Porter stemmer algorithm \cite{porter1980algorithm}.

\subsubsection{Stopword Removal} \label{impl-stop}

Stopwords (such as `the', `a' and `it') are frequently used words which carry less meaning than other words in a text. In many sentiment analysis applications, stopwords will not provide any information about the sentiment of a text, due to the fact that they frequently occur in most texts (regardless of sentiment). However there are also cases where it is useful to consider stopwords in classification - for example, a lack of prepositions is often associated with less formal texts. I implemented stopword removal using the English stopwords provided by the Natural Language Toolkit. Further to this, I added functionality to whitelist words on the stopword list and add other words to the list. Listing \ref{lst:remove_stopwords} shows the simple function I implemented to remove stopwords from a given list of words.

\pylisting{A function which removes the stopwords and words on from a given list of words.}{remove_stopwords}

\subsubsection{N-grams}

The major limitation of the bag-of-words model is that it doesn't represent the order of words. For this reason, many bag-of-words implementations use n-grams instead of just individual words. An n-gram is a sequence of n words, ordered as they are in the text they were sampled from. Using n-grams enriches the representation of a text; if we know the 2-gram `British troops' is in a given text, it tells us more than just the fact that the words `British' and `troops' are both in the text since such a text will not necessarily mention British troops. N-grams also give context to words that are otherwise semantically ambiguous. For example, the stem `author' is semantically ambiguous, since it is the stem of both `author' and `authority'. However, if the stem is part of an n-gram, such as `palestinian author', in the context of the House of Commons debates related to the Iraq war (which do not discuss Palestinian fiction), `palestinian author' is clearly derived from the phrase `Palestinian authority' (or another word that is semantically related to authority). I use this example since this the 17th most pro-war n-gram used by the classifier (as shown in table \ref{table:pro-grams}). Due to these potential gains, I implemented functionality to extract n-grams from MPs' speeches.

\subsubsection{Learning Settings and Hyperparameters} \label{impl-learning}

Once I had implemented stemming, stopword removal and n-grams as well as number-grouping and L2 normalisation, I devised a way for the program to automatically determine whether or not these potential optimisations should be applied. To facilitate this, I used a settings dictionary to determine the control flow of the program, as demonstrated in listing \ref{lst:generate_word_list}. I then used stratified cross-validation to determine the best value for each of the settings. In order to speed-up the training, I limited database accesses by retrieving and storing all the necessary data at the beginning, then getting stored data in each iteration of the cross-validation.
\\
\pylisting{A function which uses given settings to produce a word list of a given text.}{generate_word_list}
\\
I used the same stratified cross-validation scheme to determine the hyperparameters. The difference in choosing hyperparameters is that they are highly dependent on each other, so sequentially determining the value each hyperparameter would not work, as it does for choosing settings. Because of this, I decided to implement a variant on a grid search, which exhaustively considers every combination of hyperparameters within a range for each parameter. Rather than using a simple grid-search, I devised a three-phase search broken into the following steps:
\begin{enumerate}
	\item Perform a standard exhaustive grid-search.
	\item If there is one set of optimal hyperparameters and the value of one of the parameters is at the extreme of the initial range for that parameter, extend the search space to include more extreme values of the parameter. Iterate this step until the optimal set of hyperparameters does not lie on the boundary of the range of any of the hyperparameters.
	\item Perform a grid search with a finer granularity around the current optimal combination of hyperparameters.
\end{enumerate}
Figure \ref{fig:gridsearch} gives a graphic illustration of how this algorithm works. The search in the diagram is over ranges for two parameters (for example, $C$ and $\gamma$ in an RBF kernel), where each row represents a particular value for one parameter and each column represents a particular value for the other. The three grids represent the of hyperparameters considered at each of the three consecutive stages of the algorithm. The shaded squares indicate the optimal combination of hyperparameters in each phase. Since the best hyperparameters in the first step of the algorithm are in the furthest right column, we extend the search space to the right, as shown in the second grid. This algorithm extends to an arbitrary number of dimensions and in practice, I used log-scales where they were most appropriate.

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.4]{figs/gridsearch.jpg}
	\end{center}
	\caption{Diagrams illustrating how the hyperparameter search is conducted.}
	\label{fig:gridsearch}
\end{figure}

\subsubsection{Dimensionality Reduction} \label{impl-reduction}

The running time of an SVM classifier increases with the size of the features. It, therefore, makes sense to use dimensionality reduction techniques to improve the efficiency of classification. In this project, I used principal component analysis (PCA), as it is a relatively efficient method and there is good library support for it. Initially, I used the Scikit-learn implementation of PCA \cite{sklearnpca}, however after testing it I found that I could perform the feature reduction more efficiently using the NumPy singular value decomposition function \cite{numpysvd} and using the resulting matrices to perform reduce the size of the features (which is how PCA is carried out \cite{svd}). This implementation also proved too slow to be practical, so I adopted an approach using sparse matrices, which significantly improved the performance. Listing \ref{lst:reduce_features} gives my Python code for this dimensionality reduction.

\pylisting{A function using NumPy array operations to perform singular value decomposition on two sets of given features}{reduce_features}

\section{Summary}

In this chapter, I described the overall system architecture and discussed the work I undertook to implement the project's components. Throughout the preparation and implementation, I made decisions to facilitate the evaluation, which is detailed further in the next chapter.

\chapter{Evaluation} \label{eval}

In this chapter, I assess the successes and failures of this project. To follow a similar order to the implementation section, I first analyse the data in \S\ref{eval-database}, then evaluate the classifier's results in \S\ref{eval-results} and in \S\ref{eval-spam} I evaluate the results for the spam email classifier. I conclude this chapter by assessing the extent to which the project met its goals in \S\ref{eval-goals}.

\section{Database Analysis} \label{eval-database}

Since the result of the data acquisition described in \S\ref{impl-acquisition} was predominantly a textual dataset, it was difficult to quantitatively analyse this part of the project, but I performed various checks in order to establish that the dataset was constructed correctly.
\\\\
Firstly, the defensive programming style I adopted when writing the code to fetch, collate and store the data was geared towards facilitating the evaluation - wherever there was an error with the data, I outputted the relevant information and didn't write the data to the database. I then manually checked these outputs to see if there were any data that should have been written to the database but were not due to the error. All of the outputs related to instances where there were problems with the Hansard, which indicates that my program worked correctly.
\\\\
I also assessed the database by performing a series of unit tests on the database class's methods. For these tests, I used the original data sources (the Hansard and data.parliament.uk) to create a series of expected outputs from a set of queries I wrote. I then ran the queries and the actual results were 100\% consistent with the expected results. Further to this, when accessing the data to develop the project's classifier, the database didn't produce any errors. Given that the primary function of the database was its use for this classifier, this supports the success of the data acquisition section of the project.
\\\\
The main quantitative analysis I carried out on the database was considering the word (and n-gram) frequency distribution. According to Zipf's law, the frequency of the $i^{th}$ most frequent term is inversely proportional to $i$ (the word's rank). More specifically, there is a linear relationship between the logarithm of the frequency of a word and the logarithm of the rank of the word \cite{zipf}. Although Zipf's law is just an empirical observation, if the database was constructed correctly, we would expect it to obey this rule. Figure \ref{fig:bigzipf} shows a scatter plot of frequency against rank for all the words in the debates considered in this project, constructed using the project's database. The graph is plotted on a log-log scale, which is why we see the linear correlation that we would expect of a natural corpus. The coefficient of determination, $r^2$, for the plotted data is $0.97 \pm 0.07$\footnote{Although the standard error suggests it could be, the coefficient of determination cannot exceed 1.}, which provides further evidence for the project's corpus obeying Zipf's law and supports my hypothesis that it is properly constructed. 
\\
\begin{figure}
	\begin{center}
		\includegraphics[scale=1]{figs/bigzipf.png}
	\end{center}
	\caption{A scatter plot of frequency against rank for all the words in the debates considered in this project.}
	\label{fig:bigzipf}
\end{figure}
\\
Figure \ref{fig:ngramoffset} is a scatter plot of probability offset against their frequency (on a log scale) for all n-grams (where $n \le 5$) that occur at least 50 times in the speeches used by the classifier. The probability offset of a given term is given by:
\begin{equation}
	\frac{\sum_{s \in P}c_{s}(\text{term})}{\sum_{s \in P \cup N}c_{s}(\text{term})} - \frac{\sum_{t \in T}\sum_{s \in P}c_{s}(t)}{\sum_{t \in T}\sum_{s \in P \cup N}c_{s}(t)}
\end{equation}
where $c_{s}(t)$ is the occurrences of term $t$ in speech $s$, $T$ is the set of the n-grams in the training set (with frequency $\ge 50$ and $n \le 5$), $P$ is the set of positively labelled (pro-war) speeches in the training set and $N$ is the set of negatively labelled (anti-war) speeches in the training set. The first part of the expression is the probability of an occurrence of the given term being in a pro-war speech, while the second part is the probability that a given occurrence of any term is in a pro-war speech. This normalisation is necessary due to the fact that the dataset is unbalanced. Any terms with a positive probability offset have a pro-war bias and any terms with a negative probability offset have an anti-war bias. The figure shows that in general, the more common terms are less biased, but due to their frequency, their biases will likely be more significant, meaning they are still very useful for classification. Tables \ref{table:pro-grams} and \ref{table:anti-grams} show the most pro-war and anti-war terms respectively. Since the classifier uses stemming, stopword removal and n-grams up to $n=5$, the terms in these tables are processed in the same way. Table \ref{table:databasestats} gives other statistics about the contents of the database, which I computed as part of the database analysis.
\\\\
\begin{figure}
	\begin{center}
		\includegraphics[scale=0.8]{figs/ngramoffset.png}
	\end{center}
	\caption{A scatter plot of probability offset against frequency for the n-grams in the training set.}
	\label{fig:ngramoffset}
\end{figure}
\begin{table}[]
	\centering
	\begin{tabular}{lll}
		\hline
		\textbf{Term} & \textbf{Probability Offset} & \textbf{Frequency} \\ \hline
		grate hon&0.233652194&55\\
		royal marin&0.229475289&74\\
		river&0.220996222&102\\
		interim&0.212323522&52\\
		chapter&0.209409769&66\\
		state israel&0.203349163&60\\
		brigad&0.201050313&58\\
		complianc&0.200571386&72\\
		board&0.199840391&57\\
		regiment&0.195941756&54\\
		novemb&0.194258254&66\\
		avail&0.190650751&189\\
		taliban regim&0.186682497&72\\
		saddam&0.182103742&91\\
		practic&0.181780536&68\\
		emphasis&0.180730116&56\\
		palestinian author&0.177232325&97\\
		reconstruct&0.175898183&85\\
		educ&0.175676207&53\\
		deliv&0.173524602&114\\ \hline
	\end{tabular}
	\caption{The 20 most pro-war n-grams that occur at least 50 times in the debates considered in this project.}
	\label{table:pro-grams}	
\end{table}
\begin{table}[]
	\centering
	\begin{tabular}{lll}
		\hline
		\textbf{Term}  & \textbf{Probability Offset} & \textbf{Frequency} \\ \hline
		war iraq&-0.621&64\\
		cluster bomb&-0.484&69\\
		cluster&-0.442&73\\
		british govern&-0.409&56\\
		oil&-0.3219&93\\
		go war&-0.291&82\\
		billion&-0.290&50\\
		mani us&-0.270&50\\
		bomb&-0.249&432\\
		georg&-0.249&81\\
		nuclear weapon&-0.246&95\\
		bush&-0.242&250\\
		drop&-0.237&67\\
		court&-0.230&104\\
		conserv&-0.230&98\\
		american&-0.219&444\\
		tell us&-0.216&74\\
		authoris&-0.215&68\\
		administr&-0.212&228\\
		fire&-0.211&77\\ \hline
	\end{tabular}
	\caption{The 20 most anti-war n-grams that occur at least 50 times in the debates considered in this project.}
	\label{table:anti-grams}
\end{table}
\begin{table}[]
	\centering
	\begin{tabular}{lll}
		\hline
		\textbf{Quantity} & \textbf{Total} & \textbf{Used by Classifier} \\ \hline
		MPs               & 655            & 276                         \\
		Speeches          & 19,513         & 2,83                        \\
		Debates           & 1578           & 36                          \\ 
		Tokens            & 4,189,328      & 592,295                     \\
		Distinct words    & 57,288         & 19,254                      \\ \hline
	\end{tabular}
	\caption{A breakdown of the numbers in the database.}
	\label{table:databasestats}	
\end{table}
\section{Classifier Results} \label{eval-results}

Table \ref{table:results} highlights the successful performance of the classifier I developed for this project - it significantly outperforms the na\"{i}ve Bayes classifier, which I decided would be the baseline in the initial stages of the project. Note that I have used F1 scores as the metric to compare classifiers, as they account for imbalances in data, as I discussed in \S\ref{evaluation-metrics}.
\\
\begin{table}[]
	\centering
	\begin{tabular}{lll}
		\hline
		\textbf{Classifier}      & \textbf{F1 score by speech} & \textbf{F1 score by MP} \\ \hline
		Na\"{i}ve Bayes baseline & 0.392                       & 0.182                   \\
		SVM                      & 0.894                       & 0.809                   \\ \hline
	\end{tabular}
	\caption{The F1 scores of the project's main classifier and the baseline classifier.}
	\label{table:results}	
\end{table}
\\
The cross-validation learning algorithm established that the RBF kernel function would likely produce the best results. In doing so, it performed a search for the best combination of hyperparameters (as described in \S\ref{impl-learning}). Figure \ref{fig:hyperparamgrid} illustrates the first step of the grid search for hyperparameters for the RBF kernel.
\\
\begin{figure}
	\begin{center}
		\includegraphics[scale=0.8]{figs/hyperparamgrid.png}
	\end{center}
	\caption{A diagram showing the grid of the first stage of the hyperparameter search for the RBF kernel.}
	\label{fig:hyperparamgrid}
\end{figure}
\\
The learning algorithm which determined the kernel and hyperparameters also determined the settings, which are shown in table \ref{table:settings}. As tables \ref{table:anti-grams} and \ref{table:pro-grams} demonstrate, n-grams were very useful in classification, so it makes sense that the learning algorithm chose to use them. Although using n-grams up to $n = 5$ produces a large number of features, this was easily handled using the dimensionality reduction I implemented. I detailed why stemming and stopword removal were useful in sections \S\ref{impl-stem} and \S\ref{impl-stop} respectively. Although it may be useful to group numbers for some applications, it was not in this case, as evidenced by the fact that `resolut 1441' was one of the most pro-war terms (the United Nations Security Council Resolution 1441 was an offering to Iraq to encourage them to disarm \cite{resolution}).
\\\\
Interestingly, table \ref{table:results} shows that for both classifiers, the F1 score was higher when calculated on predictions for each speech, rather than on predictions for each MP. This is counter-intuitive, since the MPs' predictions are made using an aggregation of their speeches - we might expect that having more data for a particular example would lead to a better classification. The most feasible explanation is that speeches made by MPs who make fewer speeches are more likely to be misclassified. This makes sense since if an MP is less sure of their stance on the war, their view is more likely to have changed over the period of time from which this project's corpus was created. Further to this, an MP who is less sure of their stance is more likely to be convinced to follow their party's line. I decided to test the hypothesis that MPs who made fewer speeches in debates related to the Iraq war were more likely to be misclassified, by carrying out a t-test. The mean number of speeches made for correctly classified MPs was 10, whereas it was only 5 for misclassified MPs. While this supports the hypothesis, the t statistic was 0.951, meaning it was only significant at the 80\% level, which is not high enough to make any conclusive statements about the hypothesis.

\begin{table}[]
	\centering
	\begin{tabular}{ll}
		\hline
		\textbf{Setting}  & \textbf{Value} \\ \hline
		n\_gram           & 5              \\ 
		remove\_stopwords & True           \\
		stem\_words       & True           \\
		group\_numbers    & False          \\
		normalise         & True           \\
		\hline
	\end{tabular}
	\caption{The settings learned by the SVM classifier.}
	\label{table:settings}	
\end{table}




\section{Spam Email Dataset} \label{eval-spam}

As discussed in \S\ref{impl-overfitting}, developing a classifier to detect spam email before the project's main classifier had many benefits, including adding a further way to evaluate the system. This evaluation is possible since the spam email dataset I used was published by Medlock along with a paper containing state-of-the-art results for spam email detection \cite{spampaper}, meaning that I could compare these results with my own. My classifier achieved 90\% accuracy\footnote{I give the accuracy rather than F1 score here, since this was the metric used in Medlock's paper.} on the spam email dataset, compared with the 93\% given in Medlock's paper for SVM classification. Since Medlock's classifier was specifically tailored to detecting spam email and mine was not (since spam detection was not the focus of this project), the 90\% accuracy my classifier achieved is a respectable result, providing further evidence for the success of this project.

\section{Evaluation of Project Goals} \label{eval-goals}

The two primary goals of the project were defined in \S\ref{prep-requirements}. They were to ``Construct a database that comprises British texts on the Iraq war'' and to ``Develop a classifier that can determine the stance of the texts in the database''. \S\ref{eval-database} provides evidence of my achievement of the first of these goals and \S\ref{eval-results} shows that I have successfully completed the second of the two goals. I refined these goals to give the project's core tasks in table \ref{table:tasks}, which I completed (as detailed in \S\ref{impl}), before going on to complete extensions to the project.

\section{Summary}

This project was successful in achieving two main aims of creating a dataset of British texts on the Iraq war and developing a classifier to use this corpus. The novel method of using an MPs' House of Commons voting record to label their speeches proved to be successful, as demonstrated by the classifier's good performance.

\chapter{Conclusions} \label{conc}

\section{Achievements}

Through carrying out this project, I created a dataset using data from the Hansard and data.parliament.uk. This dataset contains all the speeches and votes made between September 11th, 2001 and 18th March, 2003 (inclusive). I then used this dataset to develop an SVM classifier which achieved an F1 score of 0.894 on MPs' speeches, which is significantly better than the baseline 0.392, which was the F1 score of the na\"{i}ve Bayes classifier on the same data.

\section{Lessons Learned}

The success of the project demonstrates that the Hansard is a great resource for NLP applications, especially as I showed that it can be labelled using House of Commons voting data. While the project did go well overall, the licensing issues with the initial dataset delayed the project from the off. If I were to do the project again, I would check the licences of any necessary resources before starting any other work, so as to avoid a similar problem. In addition to learning more about licensing of data, I learned a lot about NLP and machine learning techniques by doing this project.

\section{Future Work}

To improve the classifier further, I could adopt more complex models which could account for how MPs' stances change over time or how likely an MP is to follow their party's line in a vote where this contradicts the MP's own views. A new model could also use additional data from other sources, such as newspapers and social media to improve classification. The dataset could be extended over a longer range, thus making it useful for analyses of a wider range of political issues. I plan to create an API for this dataset, which will hopefully encourage further applications to be built using House of Commons data, which would, in turn, increase the accountability of MPs.

\begin{comment}
This chapter is likely to be very short and it may well refer back to the Introduction. It might properly explain how you would have planned the project if starting again with the benefit of hindsight.
\end{comment}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}
\label{sec:proposal}
\input{proposal}

\end{document}

% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{placeins}
\usepackage{multirow}

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Louis Max Cley Slater}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{Sentiment Analysis of Texts on the Iraq War} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Pembroke College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}
Should be 1 page

{\large
\begin{tabular}{ll}
Name:               & \bf                        \\
College:            & \bf Pembroke College                     \\
Project Title:      & \bf Sentiment Analysis of Texts on the Iraq War \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2018  \\
Word Count:         & \bf Max 12,000  \\
Project Originator: & Louis Max Cley Slater                    \\
Supervisor:         & Dr Tamara Polajnar                    \\ 
\end{tabular}
}


\section*{Original Aims of the Project}
%Max 100 words

Due to my interest in both natural language processing and politics and a lack of previous work done in the area, I decided that I wanted to perform sentiment analysis on British political texts. After extensive research, I found a study \cite{iraq_media_study} that manually assessed the biases of British newspaper articles on the Iraq war, so decided that I would use the dataset produced by the study. I aimed to develop a program to retrieve the texts of the newspaper articles specified in the study \cite{iraq_media_study} and implement a classifier using this data and a bag of words model.


\section*{Work Completed}
%Max 100 words
 - Problem with University's licence for DowJones (and others?). Didn't cover Scraping data/API use? Be specific and add Licence agreement(s) to bibliography
 - Switched to Hansard and voting datasets. Made the data retrieval stage lengthier. Cite datasets.
 - Numbers about success of classifier
 - Add anything else completed?


\section*{Special Difficulties}
%Max 100 words (most will simply say 'None')
 - Mention the original dataset licence issues and change?
 
\newpage
\section*{Declaration}

I, Louis Max Cley Slater of Pembroke College, being a candidate for Part II of the Computer Science Tripos, hereby declare that this dissertation and the work described in it are my own work, unaided except as may be specified below, and that the dissertation does not contain material that has already been used to any substantial extent for a comparable purpose.

\bigskip
\leftline{Signed}

\medskip
\leftline{Date}

\setcounter{tocdepth}{4}
\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\pagestyle{headings}

\chapter{Introduction}
\begin{comment}
\newpage
\section{Presentation and Professional Practice}

This section is a note to me. Remove before submission.

Worth 14\%

The assessors will determine whether you have taken a professional and ethical approach in your work. In particular, they will check that you have used appropriate methods and tools, understood software licenses, deployed appropriate review and evaluation techniques and been aware of the social and ethical impact of your work. You must demonstrate a structured design approach, including high-level design planning, design-for-test, consideration of human factors and systematic evaluation including confidence metrics within your evaluation. You should explain how you would show conformance with appropriate legislation, such as that for intellectual property, data protection, human subjects and software licenses such as those for open source. Show that you understand the consequences of your project (or a more fully-formed variant of it) in terms of how it might affect commercial markets, contribute to society and/or the research community.

Regarding presentation, assessors primarily require the dissertation to be literate and tidy. It is not necessary to spend hours using an advanced graphics design package but it is necessary to write with correct grammar, in a clear and focused expository style using properly constructed sentences.

Strict adherence to the top-level arrangement described in Section 12 is regarded as part of the Presentation. Candidates who fail to put their names on the top right-hand corners of cover sheets, misunderstand the phrase “at most 100 words”, or omit the Proforma altogether, will lose marks for Presentation.

Most of the marks are scored in the five chapters in the body of the dissertation.

Assessors recognise that the precise partitioning prescribed by the five chapter headings will sometimes prove too serious a constraint. A writer might, for example, feel that it is essential to discuss some aspects of the Implementation in earlier chapters. Assessors will credit Implementation marks ahead of time in such circumstances. It is unnecessary to repeat the discussion in order to earn the marks.


\newpage
Introduction and preparation - 26\%

The Introduction should explain the principal motivation for the project. Show how the work fits into the broad area of surrounding Computer Science and give a brief survey of previous related work. It should generally be unnecessary to quote at length from technical papers or textbooks. If a simple bibliographic reference is insufficient, consign any lengthy quotation to an appendix.
\end{comment}

This dissertation describes the development of a system that uses various natural language processing techniques to analyse texts on the Iraq war. The texts are transcripts of relevant debates in the House of Commons between 11th September 2001 and 18th March 2003. Despite underlying difficulties of the task, the system produced gives consistently good results when evaluated using a variety of different techniques. The details of the implementation and evaluation are expanded upon throughout this document.

\section{Motivation}

The initial idea for this project was due to my interest in both natural language processing and politics. While computational approaches are often applied to political texts, very few studies have ever specifically concerned a British corpus (a collection of written or spoken material stored on a computer and used to find out how language is used \cite{corpus_definition}). After reading various papers that used natural language processing techniques on political corpora, I noted the most common studies concerned sentiment analysis of short texts, such as newspaper headlines or Tweets. This motivated me to investigate the task of performing sentiment analysis on longer pieces of text, to make the project more unique still.
\newline
\newline
On a personal note, the invasion of Iraq was the first political issue I engaged in; I attended anti-war protests with my mum when I was very young and have continued to closely follow the developments since then. It was an issue for which people had very strong views that were not determined by their political leanings \cite{mp_votes_bbc}, therefore making it a particularly interesting topic for sentiment analysis. Furthermore, after extensive research I couldn't find any research that carried out computational sentiment analysis which focused on war. The project I carried out was more relevant still due to the recent publication of the Chilcot Enquiry \cite{chilcot2016report} and the ongoing situation in Iraq and Syria \cite{syria_iraq_air_strikes}.

\section{Corpora}

The most essential component of any sentiment analysis system is its corpus. For this reason, the first task I carried out was to obtain the use of a relevant corpus that I was licensed to use.
 
\subsection{Newspaper Dataset}

The first potential dataset I found was that produced by Robinson, P. and Goddard, P. and Brown, R. and Taylor, P.M. in which they ``evaluated media performance during the 2003 Iraq War'' \cite{iraq_media_study}. As part of their evaluation, they manually annotated the stance of 4,893 British newspaper articles on the Iraq war. They published the resulting dataset, but it didn't contain the body of the articles - only its headline, author, newspaper and publication date. I consequently investigated resources containing the text of the relevant articles and tried to cross-reference the data from these sources with the manually annotated stance. At the time, many newspapers published different stories online and in print, meaning that I could not rely on these. A few newspapers maintain electronic archives of their printed editions on the internet, however not enough newspapers had such archives. The final resource I looked into was Dow Jones Factiva, a ``global news database'' \cite{factiva}. Upon inspection, this database contained the vast majority of the articles I needed and it was possible for me to cross-reference the articles in it with the labels annotated by Robinson, P. and Goddard, P. and Brown, R. and Taylor, P.M.. I initially accessed the dataset through the University of Cambridge's subscription. I therefore (falsely) assumed that this subscription would be sufficient for use in my project, however I later discovered that an academic licence did not permit me to use the API or to carry out text-mining. I consequently contacted Dow Jones and was told that the licence I required would cost in excess of \$20,000.

\subsection{Hansard}
\label{hansard}

After exhausting all other options, I turned my attention to the House of Commons Hansard archives, which contains transcripts of debates between members of Parliament in the Commons Chamber \cite{hansard}. Due to the licensing problems I encountered with Dow Jones Factiva \cite{factiva}, I immediately looked into the licence required to scrape data from the Hansard and found that it is covered by the Open Parliament Licence \cite{open_parliament_licence}. Since the Hansard archives are available under this licence, I was permitted to:
\begin{itemize}
	\item ``copy, publish, distribute and transmit the information''
	\item ``adapt the information''
	\item ``exploit the information commercially and non-commercially, for example, by combining it with other information, or by including it in your own product or application''
\end{itemize}

A further benefit of using the Hansard is the fact that it can be labelled using MPs' voting records - for example, if an MP voted in favour of the invasion of Iraq, all of their speeches on Iraq can be labelled as pro-war. This allowed me to develop the sentiment analysis system using a supervised learning model.

\section{Challenges} \label{intro-challenges}

There are many flaws of our `democracy' in the United Kingdom. In my opinion, one major flaw is our voting system; we have a first-past-the-post system in which each member of the electorate only gets one vote. This vote goes to a candidate who is (usually) a member of a political party \cite{current_mp_parties}. This party will often force an MP to vote in accordance with the party line, regardless of the MP's own views. This subverts democracy when MPs vote with their party line, despite making contradictory election promises \cite{gov_tracker}. This issue was particularly prevalent leading up to the Iraq war, where despite protests \cite{iraq_protests} showing the public's overwhelming opposition to the invasion of Iraq, MPs voted in favour of the invasion. Within a month of being elected as Prime Minister in 1997, Tony Blair said ``Mine is the first generation able to contemplate the possibility that we may live our entire lives without going to war or sending our children to war. That is a prize beyond value.'' \cite{blair_quotes}, just six years before encouraging his Labour Party to vote invade Iraq. As a result, many Labour MPs who were previously opposed to the invasion voted in its favour. This hypocrisy of Members of Parliament is commonplace in British politics, meaning that as the electorate, we are frequently misled by the politicians representing us and the difficulties of party politics add to this.
\newline
\newline
At first glance, the Hansard appears to be a great resource for natural language processing, however its data is poorly presented and to use it, it's necessary to scrape the data from the inconsistent web pages, which in itself presented a difficulty. Having done so, labelling the data is not necessarily as straightforward as I suggested in~\ref{hansard}, as MPs do not always vote consistently with their own views.
\newline
\newline
Manually labelling speeches is cumbersome and not possible within this project (give number of relevant speeches). Manually labelling MPs' stances is also very time consuming, as it would require a lot of research reading old newspaper articles, and even then, the stances of some of the lesser known MPs might still be ambiguous. Therefore, in this project I will have to use MPs' voting records to label their speeches. Due to reasons I have previously mentioned, MPs will not necessarily vote consistently with their own views, meaning that there will be some degree of noise in the data. This issue is particularly great for the Iraq war, since the debate split parties and the leadership of both the Labour Party and Conservative Party were in favour of the war; before the Iraq war vote on 18th March 2003, the leaders of the two largest parties, Tony Blair (Labour Party) and Iain Duncan-Smith (Conservative Party) used party whips to persuade MPs to vote to support the invasion.
\newline
\newline
One result of this project will be to provide evidence in support of one of the following hypotheses:
\begin{itemize}
	\item \emph{Hypothesis 1} \label{hypothesis_one}: MPs who feel strongly enough about a given issue to speak about it in the House of Commons will not vote on their own convictions, rather than in accordance with their Party's line.
	\item \emph{Hypothesis 2} \label{hypothesis_two}: Political Parties can influence their own MPs to vote with the party line, regardless of the views of the individual MPs.
\end{itemize}

Admittedly, viewing the issues in such a binary way is an oversimplification and how an MP's party affects their vote will differently vary on a vote-by-vote basis for different MPs. While this simplification would be naïve in a Politics dissertation, overlooking nuances will allow for greater technical discussion, relevant to Computer Science dissertation. In essence, the better the performance of the classifier produced, the greater weight will be given to Hypothesis 1 over Hypothesis 2. This noise inherent in the data presents difficulties which can be mitigated through the design of the classifier.

\section{Previous Work} \label{intro-prevwork}

While many studies into sentiment analysis have been based around political issues, since 2009 the majority of such research has concerned Tweets. The first study to use Twitter as its primary corpus was `Twitter power: Tweets as electronic word of mouth' \cite{first_twitter} and there have since been countless studies following suit. In 2010, Pak, Alexander and Paroubek, Patrick proposed that Twitter could be used to determine public opinion \cite{twitter_as_a_corpus}, which was proven true later that year when sentiment analysis of Twitter provided predictions that paralleled the results of traditional election polls for the German federal election \cite{predicting_elections_twitter}.
\newline
\newline
This focus on Twitter is useful, but since most political decisions are made in Government and not on the internet, I propose that we should scrutinise the opinions of our elected politicians more than Twitter users. Part of the reason that what our MPs do in Parliament is not considered as much as it should be is due to the inaccessibility of the House of Commons; the language used by MPs when debating is unnaturally formal, making it difficult to follow and unnecessarily long-winded, whereas Tweets are inherently short and easy to interpret. A sentiment analysis system essentially summarises a text, meaning that applying sentiment analysis to Parliamentary debates would be a useful stepping stone towards summarising a debate. Unfortunately, although it makes the project more interesting, analysing longer political texts presents more challenges than analysing Tweets, in part due to the lack of guidance from similar previous work.
\newline
\newline
In the U.S., there have been a small number of papers detailing sentiment analysis on transcripts of Congress debates \cite{rep_dem_one} \cite{rep_dem_two}. The results of these studies indicate that determining an MP's stance on the Iraq war from their speeches in the House of Commons may be possible, however these papers use transcripts to determine a politician's political party, which is likely to be more clear-cut than their stance on a particular issue.
\newline
\newline
The lack of relevant works to this project highlights its uniqueness, which is one of the principal motivations for the project.

\chapter{Preparation}

\begin{comment}
Principally, this chapter should describe the work which was undertaken before code was written, hardware built or theories worked on. It should show how the project proposal was further refined and clarified, so that the Implementation stage could go smoothly rather than by trial and error.

Throughout this chapter and indeed the whole dissertation, it is essential to demonstrate that a proper professional approach was employed.

The nature of this chapter will vary greatly from one dissertation to another but, underlining the professional approach, this chapter will very likely include a section headed “Requirements Analysis” and incorporate other references to software engineering techniques.

The chapter will cite any new programming languages and systems which had to be learnt and will mention complicated theories or algorithms which required understanding.

It is essential to declare the Starting Point (see Section 7). This states any existing codebase or materials that your project builds on. The text here can commonly be identical to the text in your proposal, but it may enlarge on it or report variations. For instance, the true starting point may have turned out to be different from that declared in the proposal and such discrepancies must be explained.
\end{comment}

There were three main stages to the preparation of the project:
\begin{enumerate}
	\item Learning about the necessary concepts and methods. This was useful as it helped me to make informed decisions about implementation decisions. This required considerable work, as most of the skills and knowledge required to undertake the project are not taught in the Cambridge BA Computer Science course and the parts that are taught are Part II courses. The time scale of the Part II project meant that I had to learn the courses ahead of the lectures. Sections 2.1 through 2.7 detail this stage of the preparation and section 2.8 is also strongly linked to this stage.
	\item Defining and planning the project. A project of this scale needs clear definition of its goals and a well defined plan designed to achieve these goals. Sections 2.9 through 2.11 detail this stage of the preparation.
	\item Selecting the tools to be used for implementation. Section 2.12 details this stage of the preparation.
\end{enumerate}

\section{Introduction to Supervised Learning} \label{supervised-learning}

A supervised learning problem the task of determining the label of a given input. This is split into two phases: Learning and predicting.
\newline
\newline
In the learning phase, the system receives inputs of feature vectors and their associated labels. A feature vector of length $k$ is usually denoted by $\mathbf{x}$ where 
\begin{equation}
	\mathbf{x} = (x_1, x_2, ..., x_k) \quad \forall i \in \mathbb{Z}^+ . \forall x_i \in \mathbb{R}.
\end{equation}
A feature vector contains encodes the information necessary to predict a label. In the context of this project, there is a feature vector for each speech we consider, which contains information about the words in the speech. The label is usually denoted by $y$. The set of values that $y$ can take varies depending on the context of the supervised learning problem. For example, in a regression problem, $y \in \mathbb{R}$. This project concerns binary classification, since we simplify the problem so that we consider all speeches to be either pro-war or anti-war. Because of this, from now on, we will only consider binary classification problems, that is where $y \in \{-1, +1\}$.
\newline
\newline
The supervised learning system creates a function $h$ that takes a feature vector as an input and outputs a label. That is
\begin{equation}
	h(\mathbf{x}) = y.
\end{equation}
This definition allows us to intuitively view each feature vector as a point in k-dimensional space. We consider each point to be either negative ($y = -1$) or positive ($y = +1$). In this analogy, $h$ is a function that determines whether a point is negative or positive, depending on where it is in the k-dimensional space. The more points that $h$ sees, the better its estimation of whether new unseen points are negative or positive.
\newline
\newline
The figure below shows a visualisation of our intuition of feature vectors, where $k = 2$. In this diagram, the supervised learning system learns a function to distinguish the '-' and '+' points. Given a new, previously unseen point, this function would be able to estimate whether it is a '-' or a '+'.
\section{Introduction to the Na\"{i}ve Bayes Classifier}

This is one of the simplest classifiers to understand and implement. It uses the assumption that all features are independent of each other:
\begin{equation}
	p(x_i | x_j) = p(x_i) \quad \forall i, j \in \mathbb{Z}^+.
\end{equation}
We say that the classifier is 'na\"{i}ve' because of this assumption. Although the assumption is very rarely true, the classifier still provides good performance \cite{ml_book_murphy}.
\newline
\newline
In addition to this assumption, the classifier uses Bayes Theorem:
\begin{equation}
	P(A | B) = \frac{P(B | A)P(A)}{P(B)}.
\end{equation}
The intuition behind the classifier is that given a set of features $\mathbf{x}$, we should assign it to the class that has the highest probability, given the set of features. Using the assumption of conditional independence and Bayes Theorem, we can compute this probability as follows:
\begin{equation}
\begin{aligned}
p(C = y | \mathbf{x}) &= \frac{p(\mathbf{x} | C = y)p(C = y)}{p(\mathbf{x})} \\
&= \frac{p(x_1, \ldots, x_k | C = y)p(C = y)}{p(\mathbf{x})} \\
&= \frac{p(x_1, \ldots, x_k, C = y)}{p(\mathbf{x})} \\
&= \frac{p(x_1 | x_2, \ldots, x_k, C = y)p(x_2, \ldots, x_k, C = y)}{p(\mathbf{x})} \\
& \vdots \\
&= \frac{p(x_1 | x_2, \ldots, x_k, C = y)p(x_2 | x_3 \ldots, x_k, C = y)\cdots p(x_k | C = y)}{p(\mathbf{x})} \\
&= \frac{p(x_1 | C = y)p(x_2 | C = y) \cdots p(x_k | C = y)}{p(\mathbf{x})} \\
&= \frac{\prod_{i = 1}^{k}p(x_i | C = y)}{p(\mathbf{x})}. \\
\end{aligned}
\end{equation}

Clearly, this shows that we can compute y using:

\begin{equation}
\begin{aligned}
y &= \underset{y}{\operatorname{argmax}}\bigg(\frac{\prod_{i = 1}^{k}p(x_i | C = y)}{p(\mathbf{x})}\bigg) \\
&= \underset{y}{\operatorname{argmax}} \displaystyle \prod_{i = 1}^{k}p(x_i | C = y). \\
\end{aligned}
\end{equation}

We can estimate each $p(x_i | C = y)$ trivially using the training data. Given that in this project I am only considering binary classifiers, where $y \in {-1, +1}$, we can write this as:

\begin{equation}
max \bigg( \displaystyle \prod_{i = 1}^{k}p(x_i | C = -1), \prod_{i = 1}^{k}p(x_i | C = +1) \bigg). \\
\end{equation}

Due to its simplicity and good performance, I will use the na\"{i}ve Bayes classifier as a baseline for my project.
\section{Introduction to Support Vector Machines}

Support vector machines (SVMs) are widely used, state-of-the-art classifiers which were designed for binary classification (although they have since been modified to work for multi-class classification) \cite{ml_book}. Since I am viewing the task of determining the sentiment of speeches on the Iraq war as a binary classification problem, using a SVM is a natural choice.
\newline
\newline
In contrast to the na\"{i}ve Bayes classifier, the SVM approach to classification is not inherently probabilistic. Instead, they are a form of maximum margin classifier. A maximum margin classifier computes a hyperplane of the form
\begin{equation} \label{eq:hyperplane}
	\mathbf{w} \cdot \mathbf{x} + b = 0.
\end{equation}
where $\mathbf{w}$ is a normal to the hyperplane. $\mathbf{w}$ and $b$ are determined by the maximisation (\ref{eq:maxdelta}) and $\mathbf{x}$ is a point on the hyperplane. This hyperplane separates the training data, so that for all positive examples
\begin{equation}
\mathbf{w} \cdot \mathbf{x} + b \ge 0
\end{equation}
and for all negative examples
\begin{equation}
\mathbf{w} \cdot \mathbf{x} + b < 0.
\end{equation}
The idea of the maximum margin classifier is that it maximises $\delta$, the distance between the hyperplane and the closest examples to it. That is, it computes
\begin{equation} \label{eq:maxdelta}
	\underset{\mathbf{w}, b}{\operatorname{argmax}}(min(\delta)).
\end{equation}
The figure below illustrates this problem in a 2D space (i.e. where $\mathbf{x} = (x_1, x_2)$)
%insert figure
\newline
\newline
To determine whether feature vector $\mathbf{x}_i$ should be positively or negatively labelled, we simply need to determine which side of the hyperplane it lies on. This gives us the decision function
\begin{equation} \label{eq:initialdecision}
	y_i =
	\begin{cases}
		+1, \quad \mathbf{w} \cdot \mathbf{u} + b \ge 0 \\
		-1, \quad \text{otherwise}
	\end{cases}
\end{equation}
where $\mathbf{u}$ is the feature vector being classified. The support vectors are defined as the training examples that lie closest to the hyperplane. From the equation of the hyperplane (\ref{eq:hyperplane}), we see that we have the freedom to scale $\mathbf{w}$ and $b$ by a constant factor without changing the hyperplane itself. We can therefore define this scaling by imposing the following constraint on all training examples for mathematical convenience:
\begin{equation}
	y_i(\mathbf{w} \cdot \mathbf{x}_i + b) - 1 \ge 0.
\end{equation}
For the support vectors, we then have
\begin{align}
	& y_i(\mathbf{w} \cdot \mathbf{x}_i + b) - 1 = 0 \label{eq:svconstraint}\\
	\implies & \mathbf{w} \cdot \mathbf{x}_i = \frac{1}{y_i} - b. \label{eq:wdotx} \\
	\implies & b = \frac{1}{y_i} - \mathbf{w} \cdot \mathbf{x}_i. \label{eq:b}
\end{align}
%Insert figure
\newline
\newline
We now need to compute the width of the margin so we can then form an expression to maximise it. The figure above gives us some intuition as to how we can achieve this. Since $\mathbf{w}$ is perpendicular to the hyperplane, $\frac{\mathbf{w}}{||\mathbf{w}||}$ must be the unit normal to the hyperplane. We can then use a positively labelled support vector, $\mathbf{x}_+$ and a negatively labelled support vector, $\mathbf{x}_-$ to get an expression for the margin width:
\begin{equation}
\begin{aligned}
	\text{Margin width} & = \frac{\mathbf{w}}{||\mathbf{w}||} \cdot (\mathbf{x}_+ - \mathbf{x}_-) \\
	& = \frac{\mathbf{w} \cdot \mathbf{x}_+ - \mathbf{w} \cdot \mathbf{x}_-}{||\mathbf{w}||}
\end{aligned}
\end{equation}
We can now substitute in the result from (\ref{eq:wdotx}) to give
\begin{equation} \label{eq:width}
\begin{aligned}
\text{Margin width} & = \frac{\big(\frac{1}{y_+} - b\big) - \big(\frac{1}{y_-} - b\big)}{||\mathbf{w}||} \\
& = \frac{\frac{1}{y_+} - \frac{1}{y_-}}{||\mathbf{w}||} \\
& = \frac{1 + 1}{||\mathbf{w}||} \\
& = \frac{2}{||\mathbf{w}||} \\
\end{aligned}
\end{equation}
Our goal is to maximise the width given by (\ref{eq:width}). For mathematical convenience, we can instead solve the equivalent problem of minimising $\frac{1}{2}||\mathbf{w}||^2$. This optimisation is subject to the constraints in (\ref{eq:svconstraint}). In order to solve this constrained optimisation problem, we must use Lagrange multipliers
\begin{equation}
	\boldsymbol{\alpha} = (\alpha_1, \alpha_2, \ldots, \alpha_n)
\end{equation}
where $n$ is the number of training examples. This results in the Lagrange function
\begin{equation} \label{eq:initialL}
	L = \frac{1}{2}||\mathbf{w}||^2 - \sum_{i = 1}^{n} \alpha_i (y_i(\mathbf{w} \cdot \mathbf{x}_i + b) - 1). 
\end{equation}
Our task is now to solve the unconstrained maximisation problem
\begin{equation} \label{eq:lagrangeoptimisation}
	(\mathbf{w}_{opt}, b_{opt}) = \underset{\mathbf{w}, b}{\operatorname{argmax}}(L)
\end{equation}
Using the Karush-Kuhn-Tucker conditions, we can show that $\alpha_i = 0$ for all feature vectors that are not support vectors \cite{ml_book}. This results in fast computation and means that after training, we only need to store the support vectors. Therefore, from now on, we will sum over $\mathcal{S}$, the set of indices corresponding to the support vectors.
\newline
\newline
In order to solve the optimisation problem defined in (\ref{eq:lagrangeoptimisation}), we must find the partial derivative of L with respect to both $\mathbf{w}$ and $b$, setting the resulting expressions to 0 (since we want to vary $\mathbf{w}$ and $b$ in order to find the maximum L).
\begin{align}
	\frac{\partial L}{\partial \mathbf{w}} & = \mathbf{w} - \sum_{i \in \mathcal{S}} \alpha_i y_i \mathbf{x}_i = 0 \\
	\implies \mathbf{w} & = \sum_{i \in \mathcal{S}} \alpha_i y_i \mathbf{x}_i \label{eq:wsum} \\
	\frac{\partial L}{\partial b} & = \sum_{i \in \mathcal{S}} \alpha_i y_i = 0 \label{eq:db}
\end{align}
We can now substitute (\ref{eq:wsum}) into (\ref{eq:initialL}) to obtain a new expression for $L$ (and simplify using (\ref{eq:db})) as follows:
\begin{equation} \label{eq:separableL}
\begin{aligned}
	L & = \frac{1}{2}(\sum_{i \in \mathcal{S}} \alpha_i y_i \mathbf{x}_i)(\sum_{j \in \mathcal{S}} \alpha_j y_j \mathbf{x}_j) - \sum_{i \in \mathcal{S}} \alpha_i y_i(\sum_{j \in \mathcal{S}} \alpha_j y_j \mathbf{x}_j) \cdot \mathbf{x}_i - b\sum_{i \in \mathcal{S}} \alpha_i y_i + \sum_{i \in \mathcal{S}} \alpha_i \\
	& = \frac{1}{2}(\sum_{i \in \mathcal{S}} \alpha_i y_i \mathbf{x}_i)(\sum_{j \in \mathcal{S}} \alpha_j y_j \mathbf{x}_j) - (\sum_{i \in \mathcal{S}} \alpha_i y_i \mathbf{x}_i)(\sum_{j \in \mathcal{S}} \alpha_j y_j \mathbf{x}_j) + \sum_{i \in \mathcal{S}} \alpha_i \\
	& = \sum_{i \in \mathcal{S}} \alpha_i - \frac{1}{2}(\sum_{i \in \mathcal{S}} \alpha_i y_i \mathbf{x}_i)(\sum_{j \in \mathcal{S}} \alpha_j y_j \mathbf{x}_j) \\
	& = \sum_{i \in \mathcal{S}} \alpha_i - \frac{1}{2}\sum_{i \in \mathcal{S}} \sum_{j \in \mathcal{S}} \alpha_i \alpha_j y_i y_j (\mathbf{x}_i \cdot \mathbf{x}_j). \\
\end{aligned}
\end{equation}
We now need to find the values $\boldsymbol{\alpha}$ which maximise L:
\begin{equation} \label{eq:maximisation}
	\boldsymbol{\alpha}_{opt} = \underset{\boldsymbol{\alpha}}{\operatorname{argmax}}(L).
\end{equation}
I won't go into the details of how to find these values, but this can be done numerically. Further to this, it can be shown that the space of $L$ is convex, so we will not find a local maximum. This is a significant advantage of using SVMs over neural networks.
\newline
\newline
We can then find $\mathbf{w}_{opt}$ by substituting $\boldsymbol{\alpha}_{opt}$ into (\ref{eq:wsum}) and using the support vectors and their labels. From this, we can find $b_{opt}$ using (\ref{eq:b}) and substituting in $\mathbf{w}_{opt}$ along with any support vector and its label. We can substitute our values for $\boldsymbol{\alpha}_{opt}$ and $b_{opt}$ into the initial decision rule to obtain a new decision rule:
\begin{equation} \label{eq:separabledecision}
	y_i =
	\begin{cases}
		+1, \quad \displaystyle \sum_{i \in \mathcal{S}} y_i(\boldsymbol{\alpha}_{opt})_i (\mathbf{x}_i \cdot \mathbf{u}) + b \ge 0 \\
		-1, \quad \text{otherwise}.
	\end{cases}
\end{equation}
Thus far, we have been working under the assumption that our data is linearly separable. In practice, this is very rarely the case and for this project due to the inherent noise in our data (described in \S \ref{intro-challenges}), this assumption is very unlikely to hold.The figure below illustrates a simple example for which the data are not linearly separable.
%Insert figure
\newline
\newline
In order to fix this problem, we can use a transformation, $\phi$, to transform our feature vectors into a new space in which our data is more easily separable. Applying this transformation to (\ref{eq:separableL}) gives us a new $L$:
\begin{equation} \label{eq:phiL}
	L = \sum_{i \in \mathcal{S}} \alpha_i - \frac{1}{2}\sum_{i \in \mathcal{S}} \sum_{j \in \mathcal{S}} \alpha_i \alpha_j y_i y_j (\phi (\mathbf{x}_i) \cdot \phi (\mathbf{x}_j)).
\end{equation}
We maximise this as before, finding a new $\boldsymbol{\alpha}_{opt}$ from (\ref{eq:maximisation}) and then using those values to find $b_{opt}$. We can then use these values of $\boldsymbol{\alpha}_{opt}$ and $b_{opt}$ along with the transformation $\phi$ to obtain another decision rule:
\begin{equation} \label{eq:phidecision}
	y_i =
	\begin{cases}
		+1, \quad \displaystyle \sum_{i \in \mathcal{S}} y_i(\boldsymbol{\alpha}_{opt})_i (\phi (\mathbf{x}_i) \cdot \phi (\mathbf{u})) + b \ge 0 \\
		-1, \quad \text{otherwise}.
	\end{cases}
\end{equation}
We are yet to define $\phi$, but if we consider the contexts in which it is used, we see that it is always in the form $\phi (\mathbf{x}) \cdot \phi (\mathbf{x}')$. Therefore, rather than define $\phi$ itself, we define a kernel function
\begin{equation}
	k(\mathbf{x}, \mathbf{x}') = \phi (\mathbf{x}) \cdot \phi (\mathbf{x}').
\end{equation}
From this, we can rewrite (\ref{eq:phiL}) and (\ref{eq:phidecision}) as:
\begin{equation}
	L = \sum_{i \in \mathcal{S}} \alpha_i - \frac{1}{2}\sum_{i \in \mathcal{S}} \sum_{j \in \mathcal{S}} \alpha_i \alpha_j y_i y_j k(\mathbf{x}_i, \mathbf{x}_j) \\
\end{equation}
\begin{equation}
	y_i =
	\begin{cases}
		+1, \quad \displaystyle \sum_{i \in \mathcal{S}} y_i(\boldsymbol{\alpha}_{opt})_i k(\mathbf{x}_i, \mathbf{u}) + b \ge 0 \\
		-1, \quad \text{otherwise}.
	\end{cases}
\end{equation}
The choice of kernel function can have a significant effect on the performance of a SVM. In order to ensure good results, I will train the SVM using different kernel functions, so the classifier learns which kernel function will work best for the data. The three kernel functions I will consider are:
\begin{align}
	k(\mathbf{x}, \mathbf{x}') & = \mathbf{x} \cdot \mathbf{x}'& \label{eq:linearkernel} \\
	k(\mathbf{x}, \mathbf{x}') & = (\gamma (\mathbf{x} \cdot \mathbf{x}') + r)^d \quad & \gamma \in \mathbb{R}_{> 0} . r \in \mathbb{R}_{\ge 0} . d \in \mathbb{N}_{> 0} \label{eq:polykernel} \\
	k(\mathbf{x}, \mathbf{x}') & = e^{-\gamma ||\mathbf{x} - \mathbf{x}'||^2} \quad & \gamma \in \mathbb{R}_{> 0} \label{eq:rbfkernel}
\end{align}
From now on, I will refer to (\ref{eq:linearkernel}), (\ref{eq:polykernel}) and (\ref{eq:rbfkernel}) as the linear kernel, the polynomial kernel and the radial basis function (rbf) kernel respectively. Using the linear kernel is equivalent to our SVM before we introduced the transformation $\phi$. This shows that even with kernel functions, our data may not be linearly separable. It can be shown that linear and polynomial kernels do not necessarily transform the data so that into a space for which it is linearly separable, but the rbf kernel can always map the feature vectors to a space where they are linearly separable. This means that for the linear and polynomial kernels, we may not be able to produce a classifier with the given constraints and for the rbf kernel the SVM is very susceptible to overfitting. Both of these problems can be solved by introducing soft-margins to our SVM. This means that we will allow the SVM to incorrectly classify some of the training examples. In order to do this, we introduce a parameter $C$ which trades off correct classification of training examples with a greater margin width. A greater margin width results in a smoother function, so means that the SVM is less likely to overfit. The higher the value of $C$, the more training examples the SVM will fit correctly. $C$ is a hyperparameter - that is a parameter whose value is fixed before the classifier is trained. All of the hyperparameters for each of the kernels we are considering are shown in the table below. The hyperparameter choice significantly effects the performance of the SVM, so we need an algorithm for choosing them. This is discussed further in both \S\ref{cross-validation} \& \S\ref{implementation-classifier}.
\FloatBarrier
\begin{table}[]
	\centering
	\label{hyperparametertable}
	\begin{tabular}{ll}
		\textbf{Kernel}       & \textbf{Hyperparameters} \\ \hline
		Linear                & $C$                      \\
		Polynomial            & $C$, $\gamma$, $r$, $d$  \\
		Radial basis function & $C$, $\gamma$,             
	\end{tabular}
\end{table}
\FloatBarrier
\section{Introduction to Evaluating Supervised Learning Systems} \label{intro-to-evaluating}
\subsection{Train/Test Split} \label{train-test-split}
In \S\ref{supervised-learning}, we saw how a supervised learning system is trained on one set of data (the training set), then this trained model is used to predict the labels of previously unseen data (the testing set). In order to evaluate a system, we require the actual data labels, so we can assess the accuracy of the predictions. Having training examples in the testing set results will not provide a useful measure of the system's performance, as it would unfairly reward overfitting. In order to overcome this, we must split our labelled data before we start developing a model. Using 90\% for training and 10\% for testing is the most common way to split the labelled data.
\subsection{Cross Validation} \label{cross-validation}
It is useful to split the training set into $k$ disjoint folds. Doing this means that we can iterate the process of training and evaluating without using the data set aside for testing. This is done by training on $k-1$ of the folds, then evaluating the system on the fold left out. This is repeated $k$ times, so each fold is used for evaluation exactly once. Averaging over all the folds each fold gives a reliable evaluation metric. We can use this method to determine the system's hyperparameters and any other settings that need to be determined before training. This is called cross-validation. To do this, we repeat the method described for different combinations of hyperparameters and settings and select the combination whose average evaluation metric is greatest.
\subsection{Evaluation Metrics} \label{evaluation-metrics}
Thus far, we have only spoken abstractly about an evaluation metric. There are various options and choice of the measure should be context-specific. The basis of the definitions used in most metrics are defined in the table below:
\FloatBarrier
\begin{table}[]
	\centering
	\label{true-positive-table}
	\begin{tabular}{llll}
		&                                                                    & \multicolumn{2}{c}{Prediction} \\
		& \multicolumn{1}{l|}{}                                              & \textbf{Positive}  & \textbf{Negative}  \\ \cline{2-4} 
		\multirow{2}{*}{Actual Value}&\multicolumn{1}{l|}{\textbf{Positive}} & True Positive      & False Negative     \\
		& \multicolumn{1}{l|}{\textbf{Negative}}                             & False Positive     & True Negative     
	\end{tabular}
\end{table}
\FloatBarrier
From this, we can now define the following quantities:
\begin{align}
	TP & = \text{Number of True Positives} \\
	FN & = \text{Number of False Negatives} \\
	FP & = \text{Number of False Positives} \\
	TN & = \text{Number of True Negatives}
\end{align}
Accuracy is the simplest evaluation metric. We define this as:
\begin{equation}
	\text{Accuracy} = \frac{TP + TN}{TP + FN + FP + TN}
\end{equation}
If we have an unbalanced dataset (which is the case in this project), then accuracy is not a good evaluation metric. To illustrate this, consider the case where 1\% of our data is positive and 99\% of our data is negative. If we had a classifier that always predicted that an example was negative, its accuracy would be 99\%. Since accuracy is not a useful measure for unbalanced datasets, such as the dataset I am using in this project, I will not consider it any further.
\newline
\newline
We now define two further measures:
\begin{align}
	\text{Precision} & = \frac{TP}{TP + FP} \\
	\text{Recall} & = \frac{TP}{TP + FN}
\end{align}
A good evaluation metric for an unbalanced dataset will incorporate some trade-off between precision and recall. Such a metric may give greater weighting to precision for a precision-critical task or greater weighting to recall for a recall-critical task. Since this project is neither precision-critical nor recall-critical, we can use the $F_1$ score as the evaluation metric, since the $F_1$ score is defined as the harmonic mean of precision and recall:
\begin{equation}
	F_1 = \frac{2 \times \text{precision} \times \text{recall}}{\text{precision} + \text{recall}}.
\end{equation}
\section{Introduction to the Bag-of-Words Model}

In mathematics, a bag is a synonym for a multiset - an abstract data type which is like a set, but differs in that it can contain duplicates. In this project, I will represent MPs' speeches using a bag-of-words model, meaning that the representation ignores the order of words. Despite its simplicity, the bag-of-words model is used successfully in a range of sentiment classification applications, as it can effectively capture the discourse of a text \cite{nlp_book}.
\newline
\newline
To illustrate a bag-of-words model using an example, I will use an quote from a House of Commons debate on 18th March 2003:
\begin{center}
	\textit{``The best way to avoid war is to work through the United Nations.''}
	\newline
	 - Bill Tynan (Labour Party)
\end{center}
In a simple bag-of-words implementation, where case and punctuation are ignored, this sentence would be stored as:
\begin{align*}
	\{\text{the}&: 2,\\ \text{to}&: 2,\\ \text{best}&: 1,\\ \text{way}&: 1,\\ \text{avoid}&: 1,\\ \text{war}&: 1,\\ \text{is}&: 1,\\ \text{work}&: 1,\\ \text{through}&: 1,\\ \text{united}&: 1,\\ \text{nations}&: 1\}.
\end{align*}
It is important to note that the order of the elements above is not relevant.
\section{Introduction to the House of Commons}


\section{Spam Email Dataset - DEADLINE : 2ND APRIL}


\section{Requirements Analysis}

In my initial~\nameref{sec:proposal} [Appendix~\ref{sec:proposal}] I outlined the project as 
\section{Implementation Approach}



\section{Software Engineering Techniques}



\section{Choice of Tools}



\section{Starting Point}


\section{Summary - DEADLINE: 4TH APRIL}


\chapter{Implementation}

\begin{comment}
Worth 40\%

This chapter should describe what was actually produced: the programs which were written, the hardware which was built or the theory which was developed. Any design strategies that looked ahead to the testing stage might profitably be referred to (the professional approach again).

Descriptions of programs may include fragments of high-level code but large chunks of code are usually best left to appendices or omitted altogether. Analogous advice applies to circuit diagrams.

Draw attention to the parts of the work which are not your own. Making effective use of powerful tools and pre-existing code is often laudable, and will count to your credit if properly reported.

It should not be necessary to give a day-by-day account of the progress of the work but major milestones may sometimes be highlighted with advantage.
\end{comment}
\section{Data Scraper}


\section{Database - DEADLINE: 6TH APRIL}


\section{Classifier} \label{implementation-classifier}


\section{Summary - DEADLINE: 11TH APRIL}


\chapter{Evaluation Code - DEADLINE: 16TH APRIL}
\chapter{Evaluation}

\begin{comment}
Evaluation and conclusions worth 20\%

This is where Assessors will be looking for signs of success and for evidence of thorough and systematic evaluation as discussed in Section 8.3. Sample output, tables of timings and photographs of workstation screens, oscilloscope traces or circuit boards may be included. A graph that does not indicate confidence intervals will generally leave a professional scientist with a negative impression.

As with code, voluminous examples of sample output are usually best left to appendices or omitted altogether.

There are some obvious questions which this chapter will address. How many of the original goals were achieved? Were they proved to have been achieved? Did the program, hardware, or theory really work?

Assessors are well aware that large programs will very likely include some residual bugs. It should always be possible to demonstrate that a program works in simple cases and it is instructive to demonstrate how close it is to working in a really ambitious case.

\end{comment}

\section{Unit Testing}


\section{Internal Evaluation - DEADLINE: 18TH APRIL}


\section{External Evaluation}


\section{Evaluation of Project Goals}


\section{Summary - DEADLINE: 20TH APRIL}


\chapter{Conclusions}

\section{Achievements}

\section{Lessons Learned}

\section{Future Work - DEADLINE: 23RD APRIL}

\chapter{Diagrams - DEADLINE: 25TH APRIL}
\begin{comment}
This chapter is likely to be very short and it may well refer back to the Introduction. It might properly explain how you would have planned the project if starting again with the benefit of hindsight.
\end{comment}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}
\label{sec:proposal}
Assessors like to see some sample code or example circuit diagrams, and appendices are the sensible places to include such items. Accordingly, software and hardware projects should incorporate appropriate appendices. Note that the 12,000 word limit does not include material in the appendices, but only in extremely unusual circumstances may appendices exceed 10-15 pages - if you feel that such unusual circumstances might apply to you you should ask your Director of Studies and Supervisor to apply to the Chairman of Examiners. It is quite in order to have no appendices. Appendices should appear between the bibliography and the project proposal.
\input{proposal}

\end{document}
